import torch
import torch.nn as nn
import torch.nn.functional as F
import scipy.io as sio
import os
from torch.utils.data import Dataset, DataLoader
import platform
from argparse import ArgumentParser
from einops import rearrange
import numbers
import numpy as np
from thop import profile
from utils import transform
import random
import torch_dct as dct
from pytorch_msssim import ssim
import matplotlib.pyplot as plt
from time import time

from utils import evaluate, transform

parser = ArgumentParser(description='LUCMT-Net')

parser.add_argument('--start_epoch', type=int, default=0, help='epoch number of start training')
parser.add_argument('--end_epoch', type=int, default=200, help='epoch number of end training')
parser.add_argument('--layer_num', type=int, default=9, help='phase number of ISTA-Net-plus')
parser.add_argument('--learning_rate', type=float, default=1e-4, help='learning rate')
parser.add_argument('--group_num', type=int, default=1, help='group number for training')
parser.add_argument('--gpu_list', type=str, default='0,1', help='gpu index')

parser.add_argument('--matrix_dir', type=str, default='sampling_matrix', help='sampling matrix directory')
parser.add_argument('--model_dir', type=str, default='model', help='trained or pre-trained model directory')
parser.add_argument('--data_dir', type=str, default='data', help='training data directory')
parser.add_argument('--data_path', type=str, default='T2', help='Path to the dataset')
parser.add_argument('--log_dir', type=str, default='log', help='log directory')
parser.add_argument('--result_dir', type=str, default='result', help='result directory')

args = parser.parse_args()

start_epoch = args.start_epoch
end_epoch = args.end_epoch
learning_rate = args.learning_rate
layer_num = args.layer_num
group_num = args.group_num
gpu_list = args.gpu_list

# w1 = torch.tensor([2.0])
# w2 = torch.tensor([3.0])
# w3 = torch.tensor([2.0])
# w4 = torch.tensor([3.0])

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = '1' 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

batch_size = 2
dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor
# train.mat fastMRI_Barin_T1_1680.mat data\Xe\Xe_train.mat
Training_data_Name = 'fastMRI_Barin_T1_1680.mat'
# /code/data/T1/train/ code/data/Xe
Training_data = sio.loadmat('/code/data/T1/train/%s' % (Training_data_Name))
Training_labels = Training_data['reconstruction_esc']

nrtrain = Training_labels.shape[0]  # number of training blocks
print('number of train is', nrtrain)

def to_3d(x):
    return rearrange(x, 'b c h w -> b (h w) c')

def to_4d(x,h,w):
    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

def zero_filled(x, mask, mod=False, norm=False):
    x_dim_0 = x.shape[0]
    x_dim_1 = x.shape[1]
    x_dim_2 = x.shape[2]
    x_dim_3 = x.shape[3]
    x = x.view(-1, x_dim_2, x_dim_3, 1)

    x_real = x
    x_imag = torch.zeros_like(x_real)
    x_complex = torch.cat([x_real, x_imag], 3)

    x_kspace = torch.fft.fft2(x_complex)
    y_kspace = x_kspace * mask
    xu = torch.fft.ifft2(y_kspace)

    if not mod:
        xu_ret = xu[:, :, :, 0:1]
    else:
        xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

    xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
    xu_ret = xu_ret.float()

    return xu_ret

class BiasFree_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(BiasFree_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return x / torch.sqrt(sigma+1e-5) * self.weight

class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

class LayerNorm(nn.Module):
    def __init__(self, dim, LayerNorm_type):
        super(LayerNorm, self).__init__()
        if LayerNorm_type =='BiasFree':
            self.body = BiasFree_LayerNorm(dim)
        else:
            self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        h, w = x.shape[-2:]
        return to_4d(self.body(to_3d(x)), h, w)

class BinaryQuantize(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, k, t):
        ctx.save_for_backward(input, k, t)
        out = torch.sigmoid(input * t)  
        out = (out >= 0.5).float()
        return out

    @staticmethod
    def backward(ctx, grad_output):
        input, k, t = ctx.saved_tensors
        grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
        return grad_input, None, None, None

class blockNL(torch.nn.Module):
    def __init__(self, channels):
        super(blockNL, self).__init__()
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        
        # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
        self.norm_x = LayerNorm(32, 'WithBias')  # ä»1æ”¹ä¸º32
        self.norm_z = LayerNorm(31, 'WithBias') 

        self.t = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
        self.p = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
        self.g1 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.g2 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»31+1=32æ”¹ä¸º31+32=63
        self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
        self.pos_emb = nn.Sequential(
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
            nn.GELU(),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
        )
        
        self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

    def forward(self, x, z, w3, w4):
        b, c, h, w = x.shape
        x0 = self.norm_x(x)  
        z0 = self.norm_z(z)  
        z1 = self.t(z0)
        b, c, h, w = z1.shape
        z1 = z1.view(b, c, -1) 
        x1 = self.p(x0)  
        x1 = x1.view(b, c, -1) 
        x2 = self.g1(x0)
        x_v = x2.view(b, c, -1) 
        z2 = self.g2(z0) 
        z_v = z2.view(b, c, -1) 

        num_heads = 4  
        x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
        z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
        x_t_heads = x1_heads.permute(0, 1, 3, 2)  
        att_heads = torch.matmul(z1_heads, x_t_heads) 
        att_heads = self.softmax(att_heads)  

        v_heads = self.w3*z_v_heads+self.w4*x_v_heads

        out_x_heads = torch.matmul(att_heads, v_heads)  
        out_x_heads = out_x_heads.view(b, c, h, w)  

        out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
        y = self.v(torch.cat([x, out_x_heads], 1))  # xæ˜¯32é€šé“ï¼Œout_x_headsæ˜¯31é€šé“ï¼Œæ‹¼æ¥åæ˜¯63é€šé“
        return y

class Atten(torch.nn.Module):
    def __init__(self, channels):
        super(Atten, self).__init__()
               
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        self.norm1 = LayerNorm(self.channels, 'WithBias')
        self.norm2 = LayerNorm(self.channels, 'WithBias')
        self.conv_qv1 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_kv = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
        self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
    def forward(self, pre, cur, w1, w2):
        b, c, h, w = pre.shape
        pre_ln = self.norm1(pre)
        cur_ln = self.norm2(cur)
        q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
        q = q.view(b, c, -1)  
        v1 = v1.view(b, c, -1)
        k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
        k = k.view(b, c, -1)
        v2 = v2.view(b, c, -1)
        
        num_heads = 4  
        q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)
        att = torch.matmul(q, k.permute(0, 1, 3, 2))  
        att = self.softmax(att)
        
        v = self.w1*v1+self.w2*v2
        
        out = torch.matmul(att, v)  
        out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
        out = self.conv_out(out) + cur

        return out

class BasicBlock(torch.nn.Module):
    def __init__(self):
        super(BasicBlock, self).__init__()

        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
        self.atten = Atten(31) 
        self.nonlo = blockNL(channels=31) 
        self.norm1 = LayerNorm(32, 'WithBias')
        self.norm2 = LayerNorm(32, 'WithBias')
        
        # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
        self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
        # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
        self.grad_module = nn.Sequential(
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1)
        )
        
        self.conv_forward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        self.conv_backward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        
        # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
        self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
    def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
        # æ‰©å±•é€šé“: 1 -> 32
        x_expanded = self.channel_expand(x)
        
        z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
        # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
        if PhiTb is not None:
            # æ‰©å±•PhiTbçš„é€šé“
            PhiTb_expanded = self.channel_expand(PhiTb)
            # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
            x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
        else:
            x_grad = x_expanded
            
        # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
        x_grad_refined = self.grad_module(x_grad)
        x_input = x_grad + x_grad_refined

        # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
        x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

        # æ®‹å·®å·ç§¯
        x = self.norm1(x_input)
        x_forward = self.conv_forward(x) + x_input
        x = self.norm2(x_forward)
        x_backward = self.conv_backward(x) + x_forward
        x_pred_expanded = x_input + x_backward

        # å‹ç¼©é€šé“: 32 -> 1
        x_pred = self.channel_compress(x_pred_expanded)

        # 2. æå–è¾…åŠ©ç‰¹å¾ä¼ ç»™ä¸‹ä¸€å±‚ (å–å‰ 31 é€šé“)
        z_out = x_pred_expanded[:, :31, :, :] 

        return x_pred, z_out # å¿…é¡»è¿”å›ä¸¤ä¸ªå€¼

        # return x_pred

# ä¸ä¼šè¢«æ‰‹åŠ¨è¦†ç›–
# ---------------------------
# æ¡ä»¶æ»¤æ³¢ (FiLM é£æ ¼)
# ---------------------------
class CondFilterV2(nn.Module):
    def __init__(self, nf=16):
        super().__init__()
        self.nf = nf

        # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
        self.head = nn.Conv2d(1, nf//4, 3, padding=1)
        self.body = nn.Sequential(
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1)
        )
        
        # CS ratio æ¡ä»¶ç¼©æ”¾
        self.scale = nn.Sequential(
            nn.Conv2d(2, nf//4, 1), 
            nn.ReLU(), 
            nn.Conv2d(nf//4, nf//4, 1)
        )
        
        self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

    def forward(self, x, cs_ratio):
        # å›¾åƒç‰¹å¾æå–
        x_head = self.head(x)
        
        # æ¡ä»¶ç¼©æ”¾
        scaled = self.scale(cs_ratio) * self.body(x_head)
        
        # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
        weights = self.tail(scaled)
        w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
        return w_D, w_G

def get_zigzag_ordered_indices(h=8, w=8, q=6):
    x, y = [], []
    x1, x2, y1, y2 = 0, 0, 0, 0
    flag = True
    while x2 < h or y1 < w:
        if flag:
            x = [*x, *range(x1, x2 - 1, -1)]
            y = [*y, *range(y1, y2 + 1)]
        else:
            x = [*x, *range(x2, x1 + 1)]
            y = [*y, *range(y2, y1 - 1, -1)]
        flag = not flag
        x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
        x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
    return x[:q], y[:q]

def get_zigzag_truncated_indices(h=8, w=8, q=6):
    if random.randint(0, 1):
        x, y = get_zigzag_ordered_indices(h, w, q)
    else:
        y, x = get_zigzag_ordered_indices(w, h, q)
    return x, y

class COSO_LUCMT(nn.Module):
    def __init__(self, LayerNo, B=32, nf=16, mode='dct_only'):
        """
        mode: 
          'dual': åŸæœ‰çš„åŒåˆ†æ”¯ (DCT + Gauss)
          'dct_only': åªæœ‰é¢‘åŸŸåˆ†æ”¯ (100% é‡‡æ ·é¢„ç®—ç»™ DCT)
          'gauss_only': åªæœ‰ç©ºé—´åˆ†æ”¯ (100% é‡‡æ ·é¢„ç®—ç»™ Gauss)
        """
        super().__init__()
        self.LayerNo = LayerNo
        self.B = B
        self.N = B * B
        
        # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ
        self.cond_filter = CondFilterV2(nf=nf)
        
        # é«˜æ–¯åˆ†æ”¯æƒé‡ (å›ºå®š)
        U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
        self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        
        # é‡å»ºç½‘ç»œ - ä¿®æ”¹ä¸ºå¤„ç†1é€šé“è¾“å…¥
        self.fe = nn.Conv2d(1, 31, 3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
        self.fe2 = nn.Conv2d(1, 31, 3, padding=1) # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
        self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

        self.mode = mode 

    def define_sampling_operators(self, x, q_G, q_DCT):
        """å®šä¹‰é‡‡æ ·å’Œé‡å»ºæ“ä½œç¬¦ï¼Œç±»ä¼¼è®ºæ–‡ä¸­çš„Aå’ŒATå‡½æ•°"""
        b, c, h, w = x.shape
        n = h * w
        h_B, w_B = h // self.B, w // self.B
        
        # éšæœºåƒç´ ç½®ä¹±
        perm = torch.randperm(n, device=x.device)
        perm_inv = torch.empty_like(perm)
        perm_inv[perm] = torch.arange(n, device=x.device)
        
        # é«˜æ–¯åˆ†æ”¯éšæœºæƒé‡
        A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        
        # åˆ›å»ºæ©ç 
        mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                 < q_G.view(b, 1)).view(b, self.N, 1, 1)
        mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                   < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        
        # è·å–DCT Zig-Zagç´¢å¼•
        DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)
        
        # å®šä¹‰é‡‡æ ·æ“ä½œ
        def A_G(z):
            z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
            return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
        def A_DCT(z):
            dct_coeff = dct.dct_2d(z, norm='ortho')
            selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
            return selected * mask_DCT
        
        def AT_G(z):
            conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
            return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
        def AT_DCT(z):
            z_full = torch.zeros(b, 1, h, w, device=x.device)
            z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
            return dct.idct_2d(z_full, norm='ortho')
        
        A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
        AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
        return A, AT, mask_G, mask_DCT

    def forward(self, x, cs_ratio_batch):
        b, c, h, w = x.shape
        
        # è®¡ç®—åŒåˆ†æ”¯æµ‹é‡æ•° (é»˜è®¤æ¯”ä¾‹: Î³_D=0.4Î³, Î³_G=0.6Î³)
        total_m = int(cs_ratio_batch[0].item() * self.N)
        if self.mode == 'dct_only':
            q_G = torch.zeros(b, device=x.device).int()
            q_DCT = torch.full((b,), total_m, device=x.device).int()
        elif self.mode == 'gauss_only':
            q_G = torch.full((b,), total_m, device=x.device).int()
            q_DCT = torch.zeros(b, device=x.device).int()
        else: # dual mode (4:6 æ¯”ä¾‹)
            q_DCT = torch.tensor([int(total_m * 0.4)] * b, device=x.device).int()
            q_G = torch.tensor([total_m - int(total_m * 0.4)] * b, device=x.device).int()
        # q_G = torch.tensor([total_measurements * 0.6] * b, device=x.device).int()
        # q_DCT = torch.tensor([total_measurements * 0.4] * b, device=x.device).int()
        
        # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
        cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
        cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
        cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
        # æ·±åº¦æ¡ä»¶æ»¤æ³¢
        w_D, w_G = self.cond_filter(x, cs_ratio)
        # æ ¹æ®æ¨¡å¼å‡†å¤‡é‡‡æ ·è¾“å…¥ (ä¿æŒé€šé“é€»è¾‘ä¸€è‡´)
        x_D = x * w_D if self.mode != 'gauss_only' else torch.zeros_like(x)
        x_G = x * w_G if self.mode != 'dct_only' else torch.zeros_like(x)
        # x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
        # x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
        # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
        A, AT, mask_G, mask_DCT = self.define_sampling_operators(x, q_G, q_DCT)
        
        # åŒåˆ†æ”¯é‡‡æ ·
        x_filtered = torch.cat([x_G, x_D], dim=1)  # [B, 2, H, W]
        y = A(x_filtered)
        
        # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
        x_init_dual = AT(y)  # [B, 2, H, W]
        
        # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
        if self.mode == 'dual':
            x_init = torch.mean(x_init_dual, dim=1, keepdim=True)
        elif self.mode == 'dct_only':
            x_init = x_init_dual[:, 1:2, :, :] # å– DCT æ”¯è·¯ç»“æœ
        else:
            x_init = x_init_dual[:, 0:1, :, :] # å– Gauss æ”¯è·¯ç»“æœ
        # x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
        # é‡å»ºç½‘ç»œ
        z_pre = self.fe(x_init)  # [B, 31, H, W]
        z_cur = self.fe2(x_init) # [B, 31, H, W]
        x_recon = x_init         # [B, 1, H, W]
        for i in range(self.LayerNo):
            x_dual, z_next = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
            x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
            z_pre = z_cur
            z_cur = z_next
            
        return x_recon, y, A, q_G, q_DCT, (w_D, w_G)


# class DataConsistency(nn.Module):
#     def __init__(self):
#         super(DataConsistency, self).__init__()

#     def forward(self, x_recon, y_measured, mask):
#         """
#         x_recon: å½“å‰è¿­ä»£é‡å»ºçš„å›¾åƒ [B, 1, H, W]
#         y_measured: åŸå§‹é‡‡æ ·åˆ°çš„ K ç©ºé—´æµ‹é‡å€¼ (å¤æ•°å½¢å¼)
#         mask: é‡‡æ ·æ©ç  [B, 1, H, W]
#         """
#         # 1. å°†å›¾åƒè½¬åˆ°é¢‘åŸŸ (FFT)
#         # å‡è®¾ x_recon æ˜¯å®æ•°å›¾åƒï¼Œfft2 ä¼šè‡ªåŠ¨å¤„ç†
#         x_kspace = torch.fft.fft2(x_recon)
        
#         # 2. æ›¿æ¢é‡‡æ ·ç‚¹ï¼šåœ¨ mask ä¸º 1 çš„åœ°æ–¹ç”¨çœŸå® yï¼Œåœ¨ 0 çš„åœ°æ–¹ç”¨é¢„æµ‹å€¼
#         # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿ y_measured å’Œ x_kspace ç»´åº¦ä¸€è‡´
#         # å¦‚æœæ˜¯æ ‡å‡† MRIï¼ŒDC å…¬å¼å¦‚ä¸‹ï¼š
#         out_kspace = x_kspace + (y_measured - x_kspace) * mask
        
#         # 3. è½¬å›å›¾åƒåŸŸ (IFFT)
#         x_res = torch.fft.ifft2(out_kspace)
#         return torch.abs(x_res) # è¿”å›å®æ•°éƒ¨åˆ†ï¼ˆå¹…å€¼ï¼‰

# def to_3d(x):
#     return rearrange(x, 'b c h w -> b (h w) c')

# def to_4d(x,h,w):
#     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

# def zero_filled(x, mask, mod=False, norm=False):
#     x_dim_0 = x.shape[0]
#     x_dim_1 = x.shape[1]
#     x_dim_2 = x.shape[2]
#     x_dim_3 = x.shape[3]
#     x = x.view(-1, x_dim_2, x_dim_3, 1)

#     x_real = x
#     x_imag = torch.zeros_like(x_real)
#     x_complex = torch.cat([x_real, x_imag], 3)

#     x_kspace = torch.fft.fft2(x_complex)
#     y_kspace = x_kspace * mask
#     xu = torch.fft.ifft2(y_kspace)

#     if not mod:
#         xu_ret = xu[:, :, :, 0:1]
#     else:
#         xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

#     xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
#     xu_ret = xu_ret.float()

#     return xu_ret

# class BiasFree_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(BiasFree_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return x / torch.sqrt(sigma+1e-5) * self.weight

# class WithBias_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(WithBias_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         mu = x.mean(-1, keepdim=True)
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

# class LayerNorm(nn.Module):
#     def __init__(self, dim, LayerNorm_type):
#         super(LayerNorm, self).__init__()
#         if LayerNorm_type =='BiasFree':
#             self.body = BiasFree_LayerNorm(dim)
#         else:
#             self.body = WithBias_LayerNorm(dim)

#     def forward(self, x):
#         h, w = x.shape[-2:]
#         return to_4d(self.body(to_3d(x)), h, w)

# class BinaryQuantize(torch.autograd.Function):
#     @staticmethod
#     def forward(ctx, input, k, t):
#         ctx.save_for_backward(input, k, t)
#         out = torch.sigmoid(input * t)  
#         out = (out >= 0.5).float()
#         return out

#     @staticmethod
#     def backward(ctx, grad_output):
#         input, k, t = ctx.saved_tensors
#         grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
#         return grad_input, None, None, None

# class blockNL(torch.nn.Module):
#     def __init__(self, channels):
#         super(blockNL, self).__init__()
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
        
#         # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
#         self.norm_x = LayerNorm(32, 'WithBias')  # ä»1æ”¹ä¸º32
#         self.norm_z = LayerNorm(31, 'WithBias') 

#         self.t = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
#         self.p = nn.Sequential(
#             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
#         self.g1 = nn.Sequential(
#             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         self.g2 = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»31+1=32æ”¹ä¸º31+32=63
#         self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
#         self.pos_emb = nn.Sequential(
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
#             nn.GELU(),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
#         )
        
#         self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

#     def forward(self, x, z):
#         b, c, h, w = x.shape
#         x0 = self.norm_x(x)  
#         z0 = self.norm_z(z)  
#         z1 = self.t(z0)
#         b, c, h, w = z1.shape
#         z1 = z1.view(b, c, -1) 
#         x1 = self.p(x0)  
#         x1 = x1.view(b, c, -1) 
#         x2 = self.g1(x0)
#         x_v = x2.view(b, c, -1) 
#         z2 = self.g2(z0) 
#         z_v = z2.view(b, c, -1) 

#         num_heads = 4  
#         x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
#         z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
#         x_t_heads = x1_heads.permute(0, 1, 3, 2)  
#         att_heads = torch.matmul(z1_heads, x_t_heads) 
#         att_heads = self.softmax(att_heads)  

#         v_heads = self.w3*z_v_heads+self.w4*x_v_heads

#         out_x_heads = torch.matmul(att_heads, v_heads)  
#         out_x_heads = out_x_heads.view(b, c, h, w)  

#         out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
#         y = self.v(torch.cat([x, out_x_heads], 1))  # xæ˜¯32é€šé“ï¼Œout_x_headsæ˜¯31é€šé“ï¼Œæ‹¼æ¥åæ˜¯63é€šé“
#         return y

# class Atten(torch.nn.Module):
#     def __init__(self, channels):
#         super(Atten, self).__init__()
               
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
#         self.norm1 = LayerNorm(self.channels, 'WithBias')
#         self.norm2 = LayerNorm(self.channels, 'WithBias')
#         self.conv_qv1 = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_kv = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
#         self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
#     def forward(self, pre, cur, w1, w2):
#         b, c, h, w = pre.shape
#         pre_ln = self.norm1(pre)
#         cur_ln = self.norm2(cur)
#         q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
#         q = q.view(b, c, -1)  
#         v1 = v1.view(b, c, -1)
#         k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
#         k = k.view(b, c, -1)
#         v2 = v2.view(b, c, -1)
        
#         num_heads = 4  
#         q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         q = torch.nn.functional.normalize(q, dim=-1)
#         k = torch.nn.functional.normalize(k, dim=-1)
#         att = torch.matmul(q, k.permute(0, 1, 3, 2))  
#         att = self.softmax(att)
        
#         v = self.w1*v1+self.w2*v2
        
#         out = torch.matmul(att, v)  
#         out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
#         out = self.conv_out(out) + cur

#         return out

# class BasicBlock(torch.nn.Module):
#     def __init__(self):
#         super(BasicBlock, self).__init__()

#         self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
#         self.atten = Atten(31) 
#         self.nonlo = blockNL(channels=31) 
#         self.norm1 = LayerNorm(32, 'WithBias')
#         self.norm2 = LayerNorm(32, 'WithBias')
        
#         # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
#         self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
#         # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
#         self.grad_module = nn.Sequential(
#             nn.Conv2d(32, 32, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(32, 32, 3, padding=1)
#         )
        
#         self.conv_forward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )
#         self.conv_backward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )

#         # æ–°å¢ï¼šé¢‘åŸŸ DC å±‚
#         self.dc_layer = DataConsistency()
        
#         # æ–°å¢ï¼šé¢‘åŸŸç‰¹å¾ç»†åŒ– (å¯é€‰ï¼Œå¢åŠ åˆ›æ–°æ€§)
#         self.kspace_conv = nn.Sequential(
#             nn.Conv2d(2, 32, 1), # 2é€šé“å› ä¸º K ç©ºé—´æœ‰å®éƒ¨è™šéƒ¨
#             nn.ReLU(),
#             nn.Conv2d(32, 2, 1)
#         )
        
#         # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
#         self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
#     def forward(self, x, z_pre, z_cur, mask_G=None, y_G=None, mask_DCT=None, y_DCT=None, PhiTb=None, DCT_indices=None):    
#     # def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
#         # --- A. å›¾åƒåŸŸå¤„ç† (ä½ åŸæœ‰çš„é€»è¾‘) ---
#         x_expanded = self.channel_expand(x)     
#         z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
#         # --- ç¬¬äºŒæ­¥ï¼šé«˜æ–¯åˆ†æ”¯çš„åŒåŸŸä¿®æ­£ (åŸºäºæ¢¯åº¦çš„ DC) ---
#         # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
#         if PhiTb is not None:
#             # æ‰©å±•PhiTbçš„é€šé“ x = x + Î· * A_T(y - Ax)
#             PhiTb_expanded = self.channel_expand(PhiTb)
#             # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
#             x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
#         else:
#             x_grad = x_expanded
            
#         # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
#         x_grad_refined = self.grad_module(x_grad)
#         x_input = x_grad + x_grad_refined

#         # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
#         x_input = self.nonlo(x_input, z)

#         # æ®‹å·®å·ç§¯
#         x = self.norm1(x_input)
#         x_forward = self.conv_forward(x) + x_input
#         x = self.norm2(x_forward)
#         x_backward = self.conv_backward(x) + x_forward
#         x_pred_expanded = x_input + x_backward

#         # å‹ç¼©é€šé“: 32 -> 1
#         x_pred = self.channel_compress(x_pred_expanded)

#         # è·¯ B: æå–å‡ºè¾…åŠ©ç‰¹å¾ï¼Œä½œä¸ºä¸‹ä¸€è½®è¿­ä»£çš„ z_cur
#         # æˆ‘ä»¬å¯ä»¥å–å‰ 31 ä¸ªé€šé“ä½œä¸º zï¼Œæˆ–è€…å†åŠ ä¸€ä¸ªå·ç§¯æå– z
#         z_out = x_pred_expanded[:, :31, :, :] 

#          # --- B. é¢‘åŸŸå¤„ç† (è·¨åŸŸèåˆå…³é”®ç‚¹) ---
#         if y_DCT is not None and DCT_indices is not None:
#             # 1. è½¬åˆ° DCT åŸŸ
#             b, _, h, w = x_pred.shape
#             x_dct = dct.dct_2d(x_pred, norm='ortho')
#             # å°† y_DCT (æµ‹é‡å€¼) å¡«å›åˆ°å…¨å›¾ DCT ç³»æ•°ä¸­
#             # å‡è®¾ y_DCT å±•å¹³åçš„å½¢çŠ¶ä¸ç´¢å¼•åŒ¹é…
#             idx_x, idx_y = DCT_indices
            
#             # å…ˆå…‹éš†ä¸€ä»½ï¼Œé¿å…åŸåœ°æ“ä½œé”™è¯¯
#             x_dct_new = x_dct.clone()
#             # åªæ›¿æ¢é‡‡æ ·åˆ°çš„ç‚¹
#             # y_DCT éœ€è¦æ ¹æ®ä½ çš„ A_DCT é€»è¾‘ flatten
#             x_dct_new[:, :, idx_x, idx_y] = y_DCT.view(b, 1, -1) 
            
#             # åå˜æ¢å›å›¾åƒ
#             x_final = dct.idct_2d(x_dct_new, norm='ortho')
#         else:
#             x_final = x_pred

#         return x_final, z_out # è¿”å›ä¸¤ä¸ªå€¼ï¼

# # ä¸ä¼šè¢«æ‰‹åŠ¨è¦†ç›–
# # ---------------------------
# # æ¡ä»¶æ»¤æ³¢ (FiLM é£æ ¼)
# # ---------------------------
# class CondFilterV2(nn.Module):
#     def __init__(self, nf=16):
#         super().__init__()
#         self.nf = nf

#         # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
#         self.head = nn.Conv2d(1, nf//4, 3, padding=1)
#         self.body = nn.Sequential(
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1)
#         )
        
#         # CS ratio æ¡ä»¶ç¼©æ”¾
#         self.scale = nn.Sequential(
#             nn.Conv2d(2, nf//4, 1), 
#             nn.ReLU(), 
#             nn.Conv2d(nf//4, nf//4, 1)
#         )
        
#         self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

#     def forward(self, x, cs_ratio):
#         # å›¾åƒç‰¹å¾æå–
#         x_head = self.head(x)
        
#         # æ¡ä»¶ç¼©æ”¾
#         scaled = self.scale(cs_ratio) * self.body(x_head)
        
#         # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
#         weights = self.tail(scaled)
#         w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
#         return w_D, w_G

# def get_zigzag_ordered_indices(h=8, w=8, q=6):
#     x, y = [], []
#     x1, x2, y1, y2 = 0, 0, 0, 0
#     flag = True
#     while x2 < h or y1 < w:
#         if flag:
#             x = [*x, *range(x1, x2 - 1, -1)]
#             y = [*y, *range(y1, y2 + 1)]
#         else:
#             x = [*x, *range(x2, x1 + 1)]
#             y = [*y, *range(y2, y1 - 1, -1)]
#         flag = not flag
#         x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
#         x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
#     return x[:q], y[:q]

# def get_zigzag_truncated_indices(h=8, w=8, q=6):
#     if random.randint(0, 1):
#         x, y = get_zigzag_ordered_indices(h, w, q)
#     else:
#         y, x = get_zigzag_ordered_indices(w, h, q)
#     return x, y

# class COSO_LUCMT(nn.Module):
#     def __init__(self, LayerNo, B=32, nf=16):
#         super().__init__()
#         self.LayerNo = LayerNo
#         self.B = B
#         self.N = B * B
        
#         # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ
#         self.cond_filter = CondFilterV2(nf=nf)
        
#         # é«˜æ–¯åˆ†æ”¯æƒé‡ (å›ºå®š)
#         U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
#         self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        
#         # é‡å»ºç½‘ç»œ - ä¿®æ”¹ä¸ºå¤„ç†1é€šé“è¾“å…¥
#         self.fe = nn.Conv2d(1, 31, 3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
#         self.fe2 = nn.Conv2d(1, 31, 3, padding=1) # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
#         self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

#     def define_sampling_operators(self, x, q_G, q_DCT):
#         """å®šä¹‰é‡‡æ ·å’Œé‡å»ºæ“ä½œç¬¦ï¼Œç±»ä¼¼è®ºæ–‡ä¸­çš„Aå’ŒATå‡½æ•°"""
#         b, c, h, w = x.shape
#         n = h * w
#         h_B, w_B = h // self.B, w // self.B
        
#         # éšæœºåƒç´ ç½®ä¹±
#         perm = torch.randperm(n, device=x.device)
#         perm_inv = torch.empty_like(perm)
#         perm_inv[perm] = torch.arange(n, device=x.device)
        
#         # é«˜æ–¯åˆ†æ”¯éšæœºæƒé‡
#         A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        
#         # åˆ›å»ºæ©ç 
#         mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
#                  < q_G.view(b, 1)).view(b, self.N, 1, 1)
#         mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
#                    < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        
#         # è·å–DCT Zig-Zagç´¢å¼•
#         DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)

#         # æˆ‘ä»¬åªéœ€è¦å‰ q_DCT ä¸ªç´¢å¼•ä½œä¸ºæœ‰æ•ˆé‡‡æ ·ç‚¹
#         # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥è¿”å›å…¨é‡ç´¢å¼•ï¼ŒBlock å†…éƒ¨æ ¹æ® q_DCT æˆªå–
#         # æˆ–è€…ç›´æ¥è¿”å› truncated åçš„ç´¢å¼•
#         def get_mask_indices(b_idx):
#             q = q_DCT[b_idx].item()
#             return DCT_x[:q], DCT_y[:q]
        
#         # å®šä¹‰é‡‡æ ·æ“ä½œ
#         def A_G(z):
#             z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
#             return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
#         def A_DCT(z):
#             dct_coeff = dct.dct_2d(z, norm='ortho')
#             selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
#             return selected * mask_DCT
        
#         def AT_G(z):
#             conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
#             return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
#         def AT_DCT(z):
#             z_full = torch.zeros(b, 1, h, w, device=x.device)
#             z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
#             return dct.idct_2d(z_full, norm='ortho')
        
#         A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
#         AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
#         # ä½†ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬å°† DCT_x, DCT_y ä½œä¸º metadata è¿”å›
#         return A, AT, mask_G, mask_DCT, (DCT_x, DCT_y)

#     def forward(self, x, cs_ratio_batch):
#         b, c, h, w = x.shape
        
#         # è®¡ç®—åŒåˆ†æ”¯æµ‹é‡æ•° (é»˜è®¤æ¯”ä¾‹: Î³_D=0.4Î³, Î³_G=0.6Î³)
#         total_measurements = int(cs_ratio_batch[0].item() * self.N)
#         q_G = torch.tensor([total_measurements * 0.6] * b, device=x.device).int()
#         q_DCT = torch.tensor([total_measurements * 0.4] * b, device=x.device).int()
        
#         # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
#         cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
#         cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
#         cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
#         # æ·±åº¦æ¡ä»¶æ»¤æ³¢
#         w_D, w_G = self.cond_filter(x, cs_ratio)
#         x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
#         x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
#         # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
#         A, AT, mask_G, mask_DCT, DCT_info = self.define_sampling_operators(x, q_G, q_DCT)
        
#         # åŒåˆ†æ”¯é‡‡æ ·
#         x_filtered = torch.cat([x_D, x_G], dim=1)  # [B, 2, H, W]
#         y = A(x_filtered)# y[0] æ˜¯é«˜æ–¯æµ‹é‡å€¼, y[1] æ˜¯ DCT æµ‹é‡å€¼

#         # # ğŸš© åœ¨è¿™é‡Œæ’å…¥ç¬¬äºŒè¡Œï¼šæ£€æŸ¥é‡‡æ ·å€¼é‡çº§
#         # if getattr(self, 'print_once', True): # åªæ‰“å°ä¸€æ¬¡
#         #     print(f"y_DCT (é‡‡æ ·å€¼) èŒƒå›´: {y[1].min().item():.2f} to {y[1].max().item():.2f}")
#         #     self.print_once = False # æ‰“å°å®Œè®¾ä¸º False
        
#         # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
#         x_init_dual = AT(y)  # [B, 2, H, W]
        
#         # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
#         x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
#         # é‡å»ºç½‘ç»œ
#         z_pre = self.fe(x_init)  # [B, 31, H, W]
#         z_cur = self.fe2(x_init) # [B, 31, H, W]
#         x_recon = x_init         # [B, 1, H, W]
#         for i in range(self.LayerNo):
#             # x_dual = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
#             # x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
#             # z_pre = z_cur
#             # z_cur = x_dual[:, 1:, :, :] if x_dual.shape[1] > 1 else z_cur

#             x_recon, z_next = self.fcs[i](
#                 x_recon, 
#                 z_pre, 
#                 z_cur, 
#                 mask_G=mask_G,     # å¯¹åº” mask_G (é«˜æ–¯æ©ç )
#                 y_G=y[0],          # å¯¹åº” y_G (é«˜æ–¯æµ‹é‡å€¼)
#                 mask_DCT=mask_DCT, # é¢‘ç‡åŸŸæ©ç 
#                 y_DCT=y[1],        # é¢‘ç‡åŸŸçœŸç† (å¯¹åº”ç¬¬å››ä¸ªä¼˜åŒ–)
#                 PhiTb=x_init,       # ç©ºé—´åŸŸ/é«˜æ–¯è·¯å‚è€ƒ (å¯¹åº”é«˜æ–¯åˆ†æ”¯çš„ DC)
#                 DCT_indices=DCT_info # ä¼ å…¥ç´¢å¼•
#             )       
#             # æ›´æ–° z çš„æ»‘åŠ¨çª—å£
#             z_pre = z_cur
#             z_cur = z_next 
            
#         return x_recon

model = COSO_LUCMT(layer_num, mode='dct_only')
model = model.to(device)
# ä½¿ç”¨ DataParallel æ¥åˆ†é…åˆ°å¤šä¸ªGPU
# model = nn.DataParallel(model, device_ids=[0, 1]) 

print_flag = 1  

print("Training on device:", next(model.parameters()).device)


class RandomDataset(Dataset):
    def __init__(self, data, length):
        self.data = data
        self.len = length

    def __getitem__(self, index):
        return torch.Tensor(self.data[index, :]).float()

    def __len__(self):
        return self.len
dataset=RandomDataset(Training_labels, nrtrain)
# print(dataset[0])

if (platform.system() == "Windows"):
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=0,
                             shuffle=True)
else:
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=2,
                             shuffle=True)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
# -------------------------
# æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„
# -------------------------
chosen_cs = 0.2
log_dir = os.path.join("log")
os.makedirs(log_dir, exist_ok=True)
# all:L1SSIMMeans
log_file_name = os.path.join(log_dir, f"Log_MRI_Dct_only_LUCMT_FastMRI-L1SSIMMeans_layer_{layer_num}_group_{group_num}_{chosen_cs}.txt")

model_dir = os.path.join("model", f"MRI_Dct_only_LUCMT_FastMRI-L1SSIMMeans_layer_{layer_num}_group_{group_num}_{chosen_cs}")
os.makedirs(model_dir, exist_ok=True)

if start_epoch > 0:
    pre_model_dir = model_dir
    model.load_state_dict(torch.load('%s/net_params_%d.pkl' % (pre_model_dir, start_epoch)))

# -------------------------
# CS ratio è®­ç»ƒæ¨¡å¼
# -------------------------
train_mode = "fixed"   # "random" è¡¨ç¤ºéšæœº CS ratioï¼Œ"fixed" è¡¨ç¤ºå›ºå®š
fixed_cs_ratios = [0.1, 0.2, 0.3, 0.4, 0.5]

# è®¾ç½®ç»˜å›¾åç«¯ï¼Œé˜²æ­¢åœ¨æ— ç•Œé¢çš„æœåŠ¡å™¨ä¸ŠæŠ¥é”™
plt.switch_backend('agg')

# åŠ è½½éªŒè¯é›†
Val_data = sio.loadmat(r'.\data\T1\val\fastMRI_val_T1_208.mat')
Val_labels = Val_data['reconstruction_esc'] # å‡è®¾ key æ˜¯è¿™ä¸ª
nrval = Val_labels.shape[0]

# éªŒè¯é›† DataLoader
val_loader = DataLoader(dataset=RandomDataset(Val_labels, nrval), 
                        batch_size=batch_size, shuffle=False, num_workers=0)

# ç”¨äºè®°å½•å†å²æ•°æ®
history = {
    'train_loss': [],
    'val_loss': [],
    'val_psnr': []
}

def plot_convergence(history, save_path):
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # ç»˜åˆ¶ Loss æ›²çº¿ (å·¦è½´)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss', color='tab:red')
    ax1.plot(history['train_loss'], label='Train Loss', color='tab:red', linestyle='--')
    ax1.plot(history['val_loss'], label='Val Loss', color='tab:red')
    ax1.tick_params(axis='y', labelcolor='tab:red')
    ax1.grid(True, which='both', linestyle='--', alpha=0.5)

    # åˆ›å»ºå³è½´ç»˜åˆ¶ PSNR
    ax2 = ax1.twinx()
    ax2.set_ylabel('PSNR(dB)', color='tab:blue')
    ax2.plot(history['val_psnr'], label='Val PSNR', color='tab:blue', linewidth=2)
    ax2.tick_params(axis='y', labelcolor='tab:blue')

    fig.tight_layout()
    plt.title('Convergence Analysis (Loss and PSNR)')
    
    # åˆå¹¶å›¾ä¾‹
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

    plt.savefig(save_path)
    plt.close()

# -------------------------
# è®­ç»ƒå¾ªç¯
# -------------------------
for epoch_i in range(start_epoch + 1, end_epoch + 1):
    model.train()
    for data in rand_loader:
        # 1ï¸âƒ£ è¾“å…¥
        epoch_train_losses = []
        batch_x = data.to(device)
        # print(batch_x.shape)
        batch_x = batch_x.view(batch_x.shape[0], 1, batch_x.shape[1], batch_x.shape[2])
        batch_x, mean, std = transform.normalize_instance(batch_x, eps=1e-11)
        batch_x = batch_x.clamp(-6, 6)

        # # ğŸš© åœ¨è¿™é‡Œæ’å…¥ç¬¬ä¸€è¡Œï¼šæ£€æŸ¥å›¾åƒé‡çº§
        # print(f"--- æ•°å€¼æ£€æŸ¥ ---")
        # print(f"batch_x (å›¾åƒ) èŒƒå›´: {batch_x.min().item():.2f} to {batch_x.max().item():.2f}")

        cs_ratio_value = chosen_cs  # scalar

        # è½¬æˆ tensorï¼Œæ”¾åˆ° GPU
        cs_ratio_tensor = torch.tensor([[cs_ratio_value]], device=device).float()
        cs_ratio_batch = cs_ratio_tensor.expand(batch_x.shape[0], -1)  # [B,1]

        # 3ï¸âƒ£ forward
        # x_recon = model(batch_x, cs_ratio_batch)  # æ³¨æ„è¿™é‡Œä¼  tensor
        x_recon, y_true, A, q_G, q_DCT, (w_D, w_G) = model(batch_x, cs_ratio_batch)

        # loss_all = torch.mean(torch.pow(x_recon - batch_x, 2))
        # # 4ï¸âƒ£ Loss (å°è¯•ä½¿ç”¨ L1 Loss)
        # # loss_all = torch.nn.functional.l1_loss(x_recon, batch_x) 
        # # loss_all = torch.mean((x_recon - batch_x) ** 2)
        # # loss_all = torch.mean(torch.pow(x_recon - batch_x, 2))
        # # åœ¨è®­ç»ƒå¾ªç¯ä¸­
        l1_loss = torch.nn.functional.l1_loss(x_recon, batch_x)
        d_range = batch_x.max() - batch_x.min()
        # ssim è¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼Œæ‰€ä»¥ Loss ç”¨ 1 - ssim
        # non_negative=True ç¡®ä¿ ssim ä¸ºæ­£å€¼ï¼Œå¢åŠ ç¨³å®šæ€§
        ssim_val = ssim(x_recon, batch_x, data_range=d_range, size_average=True)
        loss_pixel = l1_loss + 0.5 * (1 - ssim_val)

        # 2. æµ‹é‡ä¸€è‡´æ€§ Loss (è¡¥å¿å»æ‰çš„åŒåŸŸå±‚)
        # 2. æµ‹é‡ä¸€è‡´æ€§ Loss (Measurement Fidelity) - è´Ÿè´£ç‰©ç†ä¿çœŸ
        # ã€å…³é”®ä¿®æ­£ã€‘ï¼šå¿…é¡»åº”ç”¨é‡‡æ ·æ—¶çš„æ¡ä»¶æƒé‡
        x_recon_D = x_recon * w_D # å¯¹é‡å»ºå›¾æ–½åŠ åŒæ ·çš„ DCT åˆ†æ”¯æƒé‡
        x_recon_G = x_recon * w_G # å¯¹é‡å»ºå›¾æ–½åŠ åŒæ ·çš„ Gaussian åˆ†æ”¯æƒé‡
        y_recon = A(torch.cat([x_recon_D, x_recon_G], dim=1)) # æ¨¡æ‹Ÿå½“æ—¶çš„ç‰©ç†é‡‡æ ·

        # y_true æ˜¯å‰å‘ä¼ æ’­æ—¶é‡‡æ ·å¾—åˆ°çš„çœŸå®æµ‹é‡å€¼
        loss_meas = torch.nn.functional.mse_loss(y_recon[0], y_true[0]) + \
                    torch.nn.functional.mse_loss(y_recon[1], y_true[1])

        # 3. é¢‘åŸŸæ„ŸçŸ¥æŸå¤±
        loss_dct = torch.nn.functional.l1_loss(dct.dct_2d(x_recon, norm='ortho'), 
                                                dct.dct_2d(batch_x, norm='ortho'))

        # æ€» Loss å¼•å¯¼
        loss_all = loss_pixel + 0.1 * loss_meas + 0.1 * loss_dct
        # loss_all = l1_loss

        # 5ï¸âƒ£ backward
        optimizer.zero_grad()
        loss_all.backward()
        optimizer.step()

        epoch_train_losses.append(loss_all.item())

        # 6ï¸âƒ£ æ‰“å°è®­ç»ƒä¿¡æ¯
        msg = (f"[{epoch_i:02d}/{end_epoch:02d}] "
               f"CS={cs_ratio_value:.2f} Loss={loss_all.item():.5f}")
        print(msg)

        with open(log_file_name, "a") as f:
            f.write(msg + "\n")

    # --- Validation Phase (æ¯ä¸ª Epoch ç»“æŸåè·‘ä¸€æ¬¡) ---
    model.eval()
    epoch_val_losses = []
    epoch_val_psnr = []
    
    with torch.no_grad():
        for data in val_loader:
            batch_x_val = data.to(device)
            if batch_x_val.dim() == 2: batch_x_val = batch_x_val.view(-1, 1, 256, 256)
            elif batch_x_val.dim() == 3: batch_x_val = batch_x_val.unsqueeze(1)

            batch_x_val, mean, std = transform.normalize_instance(batch_x_val, eps=1e-11)
            batch_x_val = batch_x_val.clamp(-6, 6)

            cs_ratio_v = torch.tensor([[chosen_cs]], device=device).expand(batch_x_val.shape[0], -1)
            
            x_res, _, _, _, _, _  = model(batch_x_val, cs_ratio_v)
            
            # è®¡ç®—éªŒè¯ Loss
            v_loss = torch.nn.functional.l1_loss(x_res, batch_x_val)
            epoch_val_losses.append(v_loss.item())
            
            # è®¡ç®—éªŒè¯ PSNR (åœ¨å½’ä¸€åŒ–åŸŸè®¡ç®—å³å¯ï¼Œç”¨äºè§‚å¯Ÿæ”¶æ•›æ€§)
            # æˆ–è€…åå½’ä¸€åŒ–è®¡ç®—æ›´å‡†ï¼Œè¿™é‡Œæ¨èåœ¨å½’ä¸€åŒ–åŸŸç®—ï¼Œé€Ÿåº¦å¿«
            for b in range(x_res.shape[0]):
                cur_psnr = evaluate.psnr(x_res[b,0].cpu().numpy(), batch_x_val[b,0].cpu().numpy())
                epoch_val_psnr.append(cur_psnr)

    # è®°å½•å¹¶ä¿å­˜å†å²
    history['train_loss'].append(np.mean(epoch_train_losses))
    history['val_loss'].append(np.mean(epoch_val_losses))
    history['val_psnr'].append(np.mean(epoch_val_psnr))

    # --- 2. åŠ¨æ€å‘½åæ–‡ä»¶ (å°† 0.4 æ›¿æ¢ä¸º {chosen_cs}) ---
    # ä½¿ç”¨ f-string è‡ªåŠ¨å¡«å……å½“å‰çš„é‡‡æ ·ç‡
    plot_filename = f"Convergence_Analysis_Dct_only_LUCMT_FastMRI-L1SSIMMeans-CS{chosen_cs}.png"
    mat_filename = f"training_history_Dct_only_LUCMT_FastMRI-L1SSIMMeans-CS{chosen_cs}.mat"

    # --- 3. æ‰§è¡Œç»˜å›¾ä¸ä¿å­˜ ---
    # è¿™æ ·å¦‚æœä½ è·‘ CS=0.1ï¼Œæ–‡ä»¶åå°±æ˜¯ ...-CS0.1.pngï¼›è·‘ 0.4 å°±æ˜¯ ...-CS0.4.png
    plot_convergence(history, os.path.join(log_dir, plot_filename))
    
    # åŒæ—¶ä¹Ÿä¿å­˜ä¸€ä»½æ•°æ®ï¼Œæ–¹ä¾¿ä»¥åç”¨ Origin æˆ– Excel é‡æ–°ç”»å›¾
    sio.savemat(os.path.join(log_dir, mat_filename), history)
    
    print(f"Epoch {epoch_i} Summary: Train Loss: {history['train_loss'][-1]:.4f}, "
            f"Val Loss: {history['val_loss'][-1]:.4f}, Val PSNR: {history['val_psnr'][-1]:.2f}")

    # 7ï¸âƒ£ ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), f"{model_dir}/net_params_{epoch_i}.pkl")

# # # åˆ†è¾¨ç‡æ— å…³çš„DISCOå·ç§¯å±‚
# # class DISCOConv2d(nn.Module):
# #     def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, 
# #                  radius=0.1, basis_functions=9, domain_range=(-1, 1)):
# #         super(DISCOConv2d, self).__init__()
# #         self.in_channels = in_channels
# #         self.out_channels = out_channels
# #         self.kernel_size = kernel_size
# #         self.stride = stride
# #         self.padding = padding
        
# #         # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
# #         self.radius = radius
# #         self.basis_functions = basis_functions
        
# #         # æ›´å¥½çš„æƒé‡åˆå§‹åŒ– - ä½¿ç”¨è¾ƒå°çš„åˆå§‹å€¼
# #         self.basis_weights = nn.Parameter(
# #             torch.randn(out_channels, in_channels, basis_functions) * 0.01
# #         )
        
# #         # åˆ›å»ºå›ºå®šçš„åŸºå‡½æ•°ç½‘æ ¼
# #         self.register_buffer('basis_grid', self.create_basis_grid())
        
# #         # åç½®é¡¹ - åˆå§‹åŒ–ä¸º0
# #         self.bias = nn.Parameter(torch.zeros(out_channels))
        
# #         # æ·»åŠ æ•°å€¼ç¨³å®šæ€§çš„å°å¸¸æ•°
# #         self.eps = 1e-8
        
# #     def create_basis_grid(self):
# #         """åˆ›å»ºåŸºå‡½æ•°ç½‘æ ¼"""
# #         grid_size = int(math.sqrt(self.basis_functions))
# #         basis_x = torch.linspace(-1, 1, grid_size)
# #         basis_y = torch.linspace(-1, 1, grid_size)
# #         basis_xx, basis_yy = torch.meshgrid(basis_x, basis_y, indexing='ij')
# #         basis_grid = torch.stack([basis_xx, basis_yy], dim=-1).reshape(-1, 2)
# #         return basis_grid
    
# #     def forward(self, x):
# #         """æ•°å€¼ç¨³å®šçš„å‰å‘ä¼ æ’­"""
# #         batch_size, channels, height_in, width_in = x.shape
        
# #         # æ£€æŸ¥è¾“å…¥
# #         if torch.isnan(x).any() or torch.isinf(x).any():
# #             print("DISCOå·ç§¯è¾“å…¥åŒ…å«NaNæˆ–Inf")
# #             return torch.zeros(batch_size, self.out_channels, height_in, width_in, device=x.device)
        
# #         # è®¡ç®—è¾“å‡ºå°ºå¯¸
# #         height_out = (height_in + 2 * self.padding - self.kernel_size) // self.stride + 1
# #         width_out = (width_in + 2 * self.padding - self.kernel_size) // self.stride + 1
        
# #         # å¯¹è¾“å…¥è¿›è¡Œå¡«å……
# #         if self.padding > 0:
# #             x_padded = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='reflect')
# #         else:
# #             x_padded = x
        
# #         # å±•å¼€è¾“å…¥ä¸ºå±€éƒ¨å—
# #         unfold = nn.Unfold(kernel_size=self.kernel_size, stride=self.stride)
# #         x_unfolded = unfold(x_padded)  # [B, C*kernel_size*kernel_size, H_out*W_out]
# #         x_unfolded = x_unfolded.view(batch_size, channels, self.kernel_size * self.kernel_size, -1)
        
# #         # ç”Ÿæˆæ ¸æƒé‡
# #         kernel_weights = self.generate_kernel_weights(x.device)
        
# #         # ä¿®å¤çŸ©é˜µä¹˜æ³•ç»´åº¦é—®é¢˜
# #         # x_unfolded: [B, C, K, N] å…¶ä¸­ K = kernel_size^2, N = H_out*W_out
# #         # kernel_weights: [1, O, C, K] å…¶ä¸­ O = out_channels
# #         # æˆ‘ä»¬éœ€è¦: [B, O, N]
        
# #         # ä½¿ç”¨einsumä¿®å¤ç»´åº¦é—®é¢˜
# #         output = torch.einsum('bcki,bock->boi', x_unfolded, kernel_weights)
        
# #         # æ·»åŠ åç½®å¹¶é‡å¡‘
# #         output = output + self.bias.unsqueeze(-1)
# #         output = output.reshape(batch_size, self.out_channels, height_out, width_out)
        
# #   import torch
import torch.nn as nn
import torch.nn.functional as F
import scipy.io as sio
import os
from torch.utils.data import Dataset, DataLoader
import platform
from argparse import ArgumentParser
from einops import rearrange
import numbers
import numpy as np
from utils import transform
import random
import torch_dct as dct
from pytorch_msssim import ssim
import matplotlib.pyplot as plt
from utils import evaluate, transform

parser = ArgumentParser(description='D2DUN')

parser.add_argument('--start_epoch', type=int, default=0, help='epoch number of start training')
parser.add_argument('--end_epoch', type=int, default=200, help='epoch number of end training')
parser.add_argument('--layer_num', type=int, default=9, help='phase number of ISTA-Net-plus')
parser.add_argument('--learning_rate', type=float, default=1e-4, help='learning rate')
parser.add_argument('--group_num', type=int, default=1, help='group number for training')
parser.add_argument('--gpu_list', type=str, default='0,1', help='gpu index')

parser.add_argument('--matrix_dir', type=str, default='sampling_matrix', help='sampling matrix directory')
parser.add_argument('--model_dir', type=str, default='model', help='trained or pre-trained model directory')
parser.add_argument('--data_dir', type=str, default='data', help='training data directory')
parser.add_argument('--data_path', type=str, default='T2', help='Path to the dataset')
parser.add_argument('--log_dir', type=str, default='log', help='log directory')
parser.add_argument('--result_dir', type=str, default='result', help='result directory')

args = parser.parse_args()

start_epoch = args.start_epoch
end_epoch = args.end_epoch
learning_rate = args.learning_rate
layer_num = args.layer_num
group_num = args.group_num
gpu_list = args.gpu_list

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = '0' 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

batch_size = 2
dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor
Training_data_Name = 'fastMRI_Barin_T1_1680.mat'
Training_data = sio.loadmat('./data/T1/train/%s' % (Training_data_Name))
Training_labels = Training_data['reconstruction_esc']

nrtrain = Training_labels.shape[0]  
print('number of train is', nrtrain)

def to_3d(x):
    return rearrange(x, 'b c h w -> b (h w) c')

def to_4d(x,h,w):
    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

def zero_filled(x, mask, mod=False, norm=False):
    x_dim_0 = x.shape[0]
    x_dim_1 = x.shape[1]
    x_dim_2 = x.shape[2]
    x_dim_3 = x.shape[3]
    x = x.view(-1, x_dim_2, x_dim_3, 1)

    x_real = x
    x_imag = torch.zeros_like(x_real)
    x_complex = torch.cat([x_real, x_imag], 3)

    x_kspace = torch.fft.fft2(x_complex)
    y_kspace = x_kspace * mask
    xu = torch.fft.ifft2(y_kspace)

    if not mod:
        xu_ret = xu[:, :, :, 0:1]
    else:
        xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

    xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
    xu_ret = xu_ret.float()

    return xu_ret

class BiasFree_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(BiasFree_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return x / torch.sqrt(sigma+1e-5) * self.weight

class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

class LayerNorm(nn.Module):
    def __init__(self, dim, LayerNorm_type):
        super(LayerNorm, self).__init__()
        if LayerNorm_type =='BiasFree':
            self.body = BiasFree_LayerNorm(dim)
        else:
            self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        h, w = x.shape[-2:]
        return to_4d(self.body(to_3d(x)), h, w)

class BinaryQuantize(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, k, t):
        ctx.save_for_backward(input, k, t)
        out = torch.sigmoid(input * t)  
        out = (out >= 0.5).float()
        return out

    @staticmethod
    def backward(ctx, grad_output):
        input, k, t = ctx.saved_tensors
        grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
        return grad_input, None, None, None

class blockNL(torch.nn.Module):
    def __init__(self, channels):
        super(blockNL, self).__init__()
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        self.norm_x = LayerNorm(32, 'WithBias')  
        self.norm_z = LayerNorm(31, 'WithBias') 

        self.t = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.p = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.g1 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.g2 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
        self.pos_emb = nn.Sequential(
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
            nn.GELU(),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
        )
        
        self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

    def forward(self, x, z, w3, w4):
        b, c, h, w = x.shape
        x0 = self.norm_x(x)  
        z0 = self.norm_z(z)  
        z1 = self.t(z0)
        b, c, h, w = z1.shape
        z1 = z1.view(b, c, -1) 
        x1 = self.p(x0)  
        x1 = x1.view(b, c, -1) 
        x2 = self.g1(x0)
        x_v = x2.view(b, c, -1) 
        z2 = self.g2(z0) 
        z_v = z2.view(b, c, -1) 

        num_heads = 4  
        x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
        z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
        x_t_heads = x1_heads.permute(0, 1, 3, 2)  
        att_heads = torch.matmul(z1_heads, x_t_heads) 
        att_heads = self.softmax(att_heads)  

        v_heads = self.w3*z_v_heads+self.w4*x_v_heads

        out_x_heads = torch.matmul(att_heads, v_heads)  
        out_x_heads = out_x_heads.view(b, c, h, w)  

        out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
        y = self.v(torch.cat([x, out_x_heads], 1)) 
        return y

class Atten(torch.nn.Module):
    def __init__(self, channels):
        super(Atten, self).__init__()
               
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        self.norm1 = LayerNorm(self.channels, 'WithBias')
        self.norm2 = LayerNorm(self.channels, 'WithBias')
        self.conv_qv1 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_kv = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
        self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
    def forward(self, pre, cur, w1, w2):
        b, c, h, w = pre.shape
        pre_ln = self.norm1(pre)
        cur_ln = self.norm2(cur)
        q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
        q = q.view(b, c, -1)  
        v1 = v1.view(b, c, -1)
        k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
        k = k.view(b, c, -1)
        v2 = v2.view(b, c, -1)
        
        num_heads = 4  
        q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)
        att = torch.matmul(q, k.permute(0, 1, 3, 2))  
        att = self.softmax(att)
        
        v = self.w1*v1+self.w2*v2
        
        out = torch.matmul(att, v)  
        out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
        out = self.conv_out(out) + cur

        return out

class BasicBlock(torch.nn.Module):
    def __init__(self):
        super(BasicBlock, self).__init__()

        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
        self.atten = Atten(31) 
        self.nonlo = blockNL(channels=31) 
        self.norm1 = LayerNorm(32, 'WithBias')
        self.norm2 = LayerNorm(32, 'WithBias')
        self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
        self.grad_module = nn.Sequential(
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1)
        )
        
        self.conv_forward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        self.conv_backward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
    def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
        x_expanded = self.channel_expand(x)
        
        z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)

        if PhiTb is not None:
            PhiTb_expanded = self.channel_expand(PhiTb)
            x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
        else:
            x_grad = x_expanded
        x_grad_refined = self.grad_module(x_grad)
        x_input = x_grad + x_grad_refined
        x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

        x = self.norm1(x_input)
        x_forward = self.conv_forward(x) + x_input
        x = self.norm2(x_forward)
        x_backward = self.conv_backward(x) + x_forward
        x_pred_expanded = x_input + x_backward
        x_pred = self.channel_compress(x_pred_expanded)
        z_out = x_pred_expanded[:, :31, :, :] 

        return x_pred, z_out 

class CondFilterV2(nn.Module):
    def __init__(self, nf=16):
        super().__init__()
        self.nf = nf
        self.head = nn.Conv2d(1, nf//4, 3, padding=1)
        self.body = nn.Sequential(
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1)
        )
        self.scale = nn.Sequential(
            nn.Conv2d(2, nf//4, 1), 
            nn.ReLU(), 
            nn.Conv2d(nf//4, nf//4, 1)
        )
        
        self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

    def forward(self, x, cs_ratio):
        x_head = self.head(x)
        scaled = self.scale(cs_ratio) * self.body(x_head)
        weights = self.tail(scaled)
        w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
        return w_D, w_G

def get_zigzag_ordered_indices(h=8, w=8, q=6):
    x, y = [], []
    x1, x2, y1, y2 = 0, 0, 0, 0
    flag = True
    while x2 < h or y1 < w:
        if flag:
            x = [*x, *range(x1, x2 - 1, -1)]
            y = [*y, *range(y1, y2 + 1)]
        else:
            x = [*x, *range(x2, x1 + 1)]
            y = [*y, *range(y2, y1 - 1, -1)]
        flag = not flag
        x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
        x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
    return x[:q], y[:q]

def get_zigzag_truncated_indices(h=8, w=8, q=6):
    if random.randint(0, 1):
        x, y = get_zigzag_ordered_indices(h, w, q)
    else:
        y, x = get_zigzag_ordered_indices(w, h, q)
    return x, y

class D2DUN(nn.Module):
    def __init__(self, LayerNo, B=32, nf=16, mode='dct_only'):
        super().__init__()
        self.LayerNo = LayerNo
        self.B = B
        self.N = B * B
        self.cond_filter = CondFilterV2(nf=nf)

        U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
        self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        self.fe = nn.Conv2d(1, 31, 3, padding=1)  
        self.fe2 = nn.Conv2d(1, 31, 3, padding=1) 
        self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

        self.mode = mode 

    def define_sampling_operators(self, x, q_G, q_DCT):
        b, c, h, w = x.shape
        n = h * w
        h_B, w_B = h // self.B, w // self.B

        perm = torch.randperm(n, device=x.device)
        perm_inv = torch.empty_like(perm)
        perm_inv[perm] = torch.arange(n, device=x.device)
        A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                 < q_G.view(b, 1)).view(b, self.N, 1, 1)
        mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                   < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)
        
        def A_G(z):
            z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
            return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
        def A_DCT(z):
            dct_coeff = dct.dct_2d(z, norm='ortho')
            selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
            return selected * mask_DCT
        
        def AT_G(z):
            conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
            return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
        def AT_DCT(z):
            z_full = torch.zeros(b, 1, h, w, device=x.device)
            z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
            return dct.idct_2d(z_full, norm='ortho')
        
        A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
        AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
        return A, AT, mask_G, mask_DCT

    def forward(self, x, cs_ratio_batch):
        b, c, h, w = x.shape
        total_m = int(cs_ratio_batch[0].item() * self.N)
        if self.mode == 'dct_only':
            q_G = torch.zeros(b, device=x.device).int()
            q_DCT = torch.full((b,), total_m, device=x.device).int()
        elif self.mode == 'gauss_only':
            q_G = torch.full((b,), total_m, device=x.device).int()
            q_DCT = torch.zeros(b, device=x.device).int()
        else: 
            q_DCT = torch.tensor([int(total_m * 0.4)] * b, device=x.device).int()
            q_G = torch.tensor([total_m - int(total_m * 0.4)] * b, device=x.device).int()

        cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
        cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
        cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        w_D, w_G = self.cond_filter(x, cs_ratio)
        x_D = x * w_D if self.mode != 'gauss_only' else torch.zeros_like(x)
        x_G = x * w_G if self.mode != 'dct_only' else torch.zeros_like(x)

        A, AT, mask_G, mask_DCT = self.define_sampling_operators(x, q_G, q_DCT)
        
        x_filtered = torch.cat([x_G, x_D], dim=1)  
        y = A(x_filtered)
        
        x_init_dual = AT(y)  
        

        if self.mode == 'dual':
            x_init = torch.mean(x_init_dual, dim=1, keepdim=True)
        elif self.mode == 'dct_only':
            x_init = x_init_dual[:, 1:2, :, :] 
        else:
            x_init = x_init_dual[:, 0:1, :, :] 
       
        z_pre = self.fe(x_init) 
        z_cur = self.fe2(x_init) 
        x_recon = x_init         
        for i in range(self.LayerNo):
            x_dual, z_next = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
            x_recon = x_dual  
            z_pre = z_cur
            z_cur = z_next
            
        return x_recon, y, A, q_G, q_DCT, (w_D, w_G)

model = D2DUN(layer_num, mode='dual')
model = model.to(device)


print_flag = 1  

print("Training on device:", next(model.parameters()).device)


class RandomDataset(Dataset):
    def __init__(self, data, length):
        self.data = data
        self.len = length

    def __getitem__(self, index):
        return torch.Tensor(self.data[index, :]).float()

    def __len__(self):
        return self.len
dataset=RandomDataset(Training_labels, nrtrain)

if (platform.system() == "Windows"):
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=0,
                             shuffle=True)
else:
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=2,
                             shuffle=True)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

chosen_cs = 0.2
log_dir = os.path.join("log")
os.makedirs(log_dir, exist_ok=True)

log_file_name = os.path.join(log_dir, f"Log_layer_{layer_num}_group_{group_num}_{chosen_cs}.txt")

model_dir = os.path.join("model", f"MRI_layer_{layer_num}_group_{group_num}_{chosen_cs}")
os.makedirs(model_dir, exist_ok=True)

if start_epoch > 0:
    pre_model_dir = model_dir
    model.load_state_dict(torch.load('%s/net_params_%d.pkl' % (pre_model_dir, start_epoch)))

train_mode = "fixed"  
fixed_cs_ratios = [0.1, 0.2, 0.3, 0.4, 0.5]

plt.switch_backend('agg')

Val_data = sio.loadmat('.\data\T1\val\fastMRI_val_T1_208.mat')
Val_labels = Val_data['reconstruction_esc'] 
nrval = Val_labels.shape[0]

val_loader = DataLoader(dataset=RandomDataset(Val_labels, nrval), 
                        batch_size=batch_size, shuffle=False, num_workers=0)

history = {
    'train_loss': [],
    'val_loss': [],
    'val_psnr': []
}


for epoch_i in range(start_epoch + 1, end_epoch + 1):
    model.train()
    for data in rand_loader:
        epoch_train_losses = []
        batch_x = data.to(device)
        batch_x = batch_x.view(batch_x.shape[0], 1, batch_x.shape[1], batch_x.shape[2])
        batch_x, mean, std = transform.normalize_instance(batch_x, eps=1e-11)
        batch_x = batch_x.clamp(-6, 6)

        cs_ratio_value = chosen_cs  

        cs_ratio_tensor = torch.tensor([[cs_ratio_value]], device=device).float()
        cs_ratio_batch = cs_ratio_tensor.expand(batch_x.shape[0], -1)  

        x_recon, y_true, A, q_G, q_DCT, (w_D, w_G) = model(batch_x, cs_ratio_batch)
        l1_loss = torch.nn.functional.l1_loss(x_recon, batch_x)
        d_range = batch_x.max() - batch_x.min()
        ssim_val = ssim(x_recon, batch_x, data_range=d_range, size_average=True)
        loss_pixel = l1_loss + 0.5 * (1 - ssim_val)

        x_recon_D = x_recon * w_D 
        x_recon_G = x_recon * w_G 
        y_recon = A(torch.cat([x_recon_D, x_recon_G], dim=1)) 

        loss_meas = torch.nn.functional.mse_loss(y_recon[0], y_true[0]) + \
                    torch.nn.functional.mse_loss(y_recon[1], y_true[1])

        loss_dct = torch.nn.functional.l1_loss(dct.dct_2d(x_recon, norm='ortho'), 
                                                dct.dct_2d(batch_x, norm='ortho'))

        loss_all = loss_pixel + 0.1 * loss_meas + 0.1 * loss_dct

        optimizer.zero_grad()
        loss_all.backward()
        optimizer.step()

        epoch_train_losses.append(loss_all.item())

        msg = (f"[{epoch_i:02d}/{end_epoch:02d}] "
               f"CS={cs_ratio_value:.2f} Loss={loss_all.item():.5f}")
        print(msg)

        with open(log_file_name, "a") as f:
            f.write(msg + "\n")

    model.eval()
    epoch_val_losses = []
    epoch_val_psnr = []
    
    with torch.no_grad():
        for data in val_loader:
            batch_x_val = data.to(device)
            if batch_x_val.dim() == 2: batch_x_val = batch_x_val.view(-1, 1, 256, 256)
            elif batch_x_val.dim() == 3: batch_x_val = batch_x_val.unsqueeze(1)

            batch_x_val, mean, std = transform.normalize_instance(batch_x_val, eps=1e-11)
            batch_x_val = batch_x_val.clamp(-6, 6)

            cs_ratio_v = torch.tensor([[chosen_cs]], device=device).expand(batch_x_val.shape[0], -1)
            
            x_res, _, _, _, _, _  = model(batch_x_val, cs_ratio_v)
            
            v_loss = torch.nn.functional.l1_loss(x_res, batch_x_val)
            epoch_val_losses.append(v_loss.item())
            
            for b in range(x_res.shape[0]):
                cur_psnr = evaluate.psnr(x_res[b,0].cpu().numpy(), batch_x_val[b,0].cpu().numpy())
                epoch_val_psnr.append(cur_psnr)
    history['train_loss'].append(np.mean(epoch_train_losses))
    history['val_loss'].append(np.mean(epoch_val_losses))
    history['val_psnr'].append(np.mean(epoch_val_psnr))
    
    print(f"Epoch {epoch_i} Summary: Train Loss: {history['train_loss'][-1]:.4f}, "
            f"Val Loss: {history['val_loss'][-1]:.4f}, Val PSNR: {history['val_psnr'][-1]:.2f}")

    torch.save(model.state_dict(), f"{model_dir}/net_params_{epoch_i}.pkl")
