import torch
import torch.nn as nn
import torch.nn.functional as F
import scipy.io as sio
import os
from torch.utils.data import Dataset, DataLoader
import platform
from argparse import ArgumentParser
from einops import rearrange
import numbers
import numpy as np
from thop import profile
from utils import transform
import random
import torch_dct as dct
from pytorch_msssim import ssim
import matplotlib.pyplot as plt
from time import time

from utils import evaluate, transform

parser = ArgumentParser(description='LUCMT-Net')

parser.add_argument('--start_epoch', type=int, default=0, help='epoch number of start training')
parser.add_argument('--end_epoch', type=int, default=200, help='epoch number of end training')
parser.add_argument('--layer_num', type=int, default=9, help='phase number of ISTA-Net-plus')
parser.add_argument('--learning_rate', type=float, default=1e-4, help='learning rate')
parser.add_argument('--group_num', type=int, default=1, help='group number for training')
parser.add_argument('--gpu_list', type=str, default='0,1', help='gpu index')

parser.add_argument('--matrix_dir', type=str, default='sampling_matrix', help='sampling matrix directory')
parser.add_argument('--model_dir', type=str, default='model', help='trained or pre-trained model directory')
parser.add_argument('--data_dir', type=str, default='data', help='training data directory')
parser.add_argument('--data_path', type=str, default='T2', help='Path to the dataset')
parser.add_argument('--log_dir', type=str, default='log', help='log directory')
parser.add_argument('--result_dir', type=str, default='result', help='result directory')

args = parser.parse_args()

start_epoch = args.start_epoch
end_epoch = args.end_epoch
learning_rate = args.learning_rate
layer_num = args.layer_num
group_num = args.group_num
gpu_list = args.gpu_list

# w1 = torch.tensor([2.0])
# w2 = torch.tensor([3.0])
# w3 = torch.tensor([2.0])
# w4 = torch.tensor([3.0])

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = '1' 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

batch_size = 2
dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor
# train.mat fastMRI_Barin_T1_1680.mat data\Xe\Xe_train.mat
Training_data_Name = 'fastMRI_Barin_T1_1680.mat'
# /code/data/T1/train/ code/data/Xe
Training_data = sio.loadmat('/code/data/T1/train/%s' % (Training_data_Name))
Training_labels = Training_data['reconstruction_esc']

nrtrain = Training_labels.shape[0]  # number of training blocks
print('number of train is', nrtrain)

def to_3d(x):
    return rearrange(x, 'b c h w -> b (h w) c')

def to_4d(x,h,w):
    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

def zero_filled(x, mask, mod=False, norm=False):
    x_dim_0 = x.shape[0]
    x_dim_1 = x.shape[1]
    x_dim_2 = x.shape[2]
    x_dim_3 = x.shape[3]
    x = x.view(-1, x_dim_2, x_dim_3, 1)

    x_real = x
    x_imag = torch.zeros_like(x_real)
    x_complex = torch.cat([x_real, x_imag], 3)

    x_kspace = torch.fft.fft2(x_complex)
    y_kspace = x_kspace * mask
    xu = torch.fft.ifft2(y_kspace)

    if not mod:
        xu_ret = xu[:, :, :, 0:1]
    else:
        xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

    xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
    xu_ret = xu_ret.float()

    return xu_ret

class BiasFree_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(BiasFree_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return x / torch.sqrt(sigma+1e-5) * self.weight

class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)
        assert len(normalized_shape) == 1
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

class LayerNorm(nn.Module):
    def __init__(self, dim, LayerNorm_type):
        super(LayerNorm, self).__init__()
        if LayerNorm_type =='BiasFree':
            self.body = BiasFree_LayerNorm(dim)
        else:
            self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        h, w = x.shape[-2:]
        return to_4d(self.body(to_3d(x)), h, w)

class BinaryQuantize(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, k, t):
        ctx.save_for_backward(input, k, t)
        out = torch.sigmoid(input * t)  
        out = (out >= 0.5).float()
        return out

    @staticmethod
    def backward(ctx, grad_output):
        input, k, t = ctx.saved_tensors
        grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
        return grad_input, None, None, None

class blockNL(torch.nn.Module):
    def __init__(self, channels):
        super(blockNL, self).__init__()
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        
        # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
        self.norm_x = LayerNorm(32, 'WithBias')  # ä»1æ”¹ä¸º32
        self.norm_z = LayerNorm(31, 'WithBias') 

        self.t = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
        self.p = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
        self.g1 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.g2 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
        )
        self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»31+1=32æ”¹ä¸º31+32=63
        self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
        self.pos_emb = nn.Sequential(
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
            nn.GELU(),
            nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
        )
        
        self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

    def forward(self, x, z, w3, w4):
        b, c, h, w = x.shape
        x0 = self.norm_x(x)  
        z0 = self.norm_z(z)  
        z1 = self.t(z0)
        b, c, h, w = z1.shape
        z1 = z1.view(b, c, -1) 
        x1 = self.p(x0)  
        x1 = x1.view(b, c, -1) 
        x2 = self.g1(x0)
        x_v = x2.view(b, c, -1) 
        z2 = self.g2(z0) 
        z_v = z2.view(b, c, -1) 

        num_heads = 4  
        x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
        z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
        x_t_heads = x1_heads.permute(0, 1, 3, 2)  
        att_heads = torch.matmul(z1_heads, x_t_heads) 
        att_heads = self.softmax(att_heads)  

        v_heads = self.w3*z_v_heads+self.w4*x_v_heads

        out_x_heads = torch.matmul(att_heads, v_heads)  
        out_x_heads = out_x_heads.view(b, c, h, w)  

        out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
        y = self.v(torch.cat([x, out_x_heads], 1))  # xæ˜¯32é€šé“ï¼Œout_x_headsæ˜¯31é€šé“ï¼Œæ‹¼æ¥åæ˜¯63é€šé“
        return y

class Atten(torch.nn.Module):
    def __init__(self, channels):
        super(Atten, self).__init__()
               
        self.channels = channels
        self.softmax = nn.Softmax(dim=-1)
        self.norm1 = LayerNorm(self.channels, 'WithBias')
        self.norm2 = LayerNorm(self.channels, 'WithBias')
        self.conv_qv1 = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_kv = nn.Sequential(
            nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
            nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
        )
        self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
        self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
    def forward(self, pre, cur, w1, w2):
        b, c, h, w = pre.shape
        pre_ln = self.norm1(pre)
        cur_ln = self.norm2(cur)
        q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
        q = q.view(b, c, -1)  
        v1 = v1.view(b, c, -1)
        k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
        k = k.view(b, c, -1)
        v2 = v2.view(b, c, -1)
        
        num_heads = 4  
        q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
        v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)
        att = torch.matmul(q, k.permute(0, 1, 3, 2))  
        att = self.softmax(att)
        
        v = self.w1*v1+self.w2*v2
        
        out = torch.matmul(att, v)  
        out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
        out = self.conv_out(out) + cur

        return out

class BasicBlock(torch.nn.Module):
    def __init__(self):
        super(BasicBlock, self).__init__()

        self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
        self.atten = Atten(31) 
        self.nonlo = blockNL(channels=31) 
        self.norm1 = LayerNorm(32, 'WithBias')
        self.norm2 = LayerNorm(32, 'WithBias')
        
        # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
        self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
        # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
        self.grad_module = nn.Sequential(
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1)
        )
        
        self.conv_forward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        self.conv_backward = nn.Sequential(
            nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
            nn.GELU(),
            nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
        )
        
        # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
        self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
    def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
        # æ‰©å±•é€šé“: 1 -> 32
        x_expanded = self.channel_expand(x)
        
        z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
        # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
        if PhiTb is not None:
            # æ‰©å±•PhiTbçš„é€šé“
            PhiTb_expanded = self.channel_expand(PhiTb)
            # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
            x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
        else:
            x_grad = x_expanded
            
        # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
        x_grad_refined = self.grad_module(x_grad)
        x_input = x_grad + x_grad_refined

        # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
        x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

        # æ®‹å·®å·ç§¯
        x = self.norm1(x_input)
        x_forward = self.conv_forward(x) + x_input
        x = self.norm2(x_forward)
        x_backward = self.conv_backward(x) + x_forward
        x_pred_expanded = x_input + x_backward

        # å‹ç¼©é€šé“: 32 -> 1
        x_pred = self.channel_compress(x_pred_expanded)

        # 2. æå–è¾…åŠ©ç‰¹å¾ä¼ ç»™ä¸‹ä¸€å±‚ (å–å‰ 31 é€šé“)
        z_out = x_pred_expanded[:, :31, :, :] 

        return x_pred, z_out # å¿…é¡»è¿”å›ä¸¤ä¸ªå€¼

        # return x_pred

# ä¸ä¼šè¢«æ‰‹åŠ¨è¦†ç›–
# ---------------------------
# æ¡ä»¶æ»¤æ³¢ (FiLM é£æ ¼)
# ---------------------------
class CondFilterV2(nn.Module):
    def __init__(self, nf=16):
        super().__init__()
        self.nf = nf

        # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
        self.head = nn.Conv2d(1, nf//4, 3, padding=1)
        self.body = nn.Sequential(
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(nf//4, nf//4, 3, padding=1)
        )
        
        # CS ratio æ¡ä»¶ç¼©æ”¾
        self.scale = nn.Sequential(
            nn.Conv2d(2, nf//4, 1), 
            nn.ReLU(), 
            nn.Conv2d(nf//4, nf//4, 1)
        )
        
        self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

    def forward(self, x, cs_ratio):
        # å›¾åƒç‰¹å¾æå–
        x_head = self.head(x)
        
        # æ¡ä»¶ç¼©æ”¾
        scaled = self.scale(cs_ratio) * self.body(x_head)
        
        # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
        weights = self.tail(scaled)
        w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
        return w_D, w_G

def get_zigzag_ordered_indices(h=8, w=8, q=6):
    x, y = [], []
    x1, x2, y1, y2 = 0, 0, 0, 0
    flag = True
    while x2 < h or y1 < w:
        if flag:
            x = [*x, *range(x1, x2 - 1, -1)]
            y = [*y, *range(y1, y2 + 1)]
        else:
            x = [*x, *range(x2, x1 + 1)]
            y = [*y, *range(y2, y1 - 1, -1)]
        flag = not flag
        x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
        x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
    return x[:q], y[:q]

def get_zigzag_truncated_indices(h=8, w=8, q=6):
    if random.randint(0, 1):
        x, y = get_zigzag_ordered_indices(h, w, q)
    else:
        y, x = get_zigzag_ordered_indices(w, h, q)
    return x, y

class COSO_LUCMT(nn.Module):
    def __init__(self, LayerNo, B=32, nf=16, mode='dct_only'):
        """
        mode: 
          'dual': åŸæœ‰çš„åŒåˆ†æ”¯ (DCT + Gauss)
          'dct_only': åªæœ‰é¢‘åŸŸåˆ†æ”¯ (100% é‡‡æ ·é¢„ç®—ç»™ DCT)
          'gauss_only': åªæœ‰ç©ºé—´åˆ†æ”¯ (100% é‡‡æ ·é¢„ç®—ç»™ Gauss)
        """
        super().__init__()
        self.LayerNo = LayerNo
        self.B = B
        self.N = B * B
        
        # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ
        self.cond_filter = CondFilterV2(nf=nf)
        
        # é«˜æ–¯åˆ†æ”¯æƒé‡ (å›ºå®š)
        U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
        self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        
        # é‡å»ºç½‘ç»œ - ä¿®æ”¹ä¸ºå¤„ç†1é€šé“è¾“å…¥
        self.fe = nn.Conv2d(1, 31, 3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
        self.fe2 = nn.Conv2d(1, 31, 3, padding=1) # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
        self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

        self.mode = mode 

    def define_sampling_operators(self, x, q_G, q_DCT):
        """å®šä¹‰é‡‡æ ·å’Œé‡å»ºæ“ä½œç¬¦ï¼Œç±»ä¼¼è®ºæ–‡ä¸­çš„Aå’ŒATå‡½æ•°"""
        b, c, h, w = x.shape
        n = h * w
        h_B, w_B = h // self.B, w // self.B
        
        # éšæœºåƒç´ ç½®ä¹±
        perm = torch.randperm(n, device=x.device)
        perm_inv = torch.empty_like(perm)
        perm_inv[perm] = torch.arange(n, device=x.device)
        
        # é«˜æ–¯åˆ†æ”¯éšæœºæƒé‡
        A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        
        # åˆ›å»ºæ©ç 
        mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                 < q_G.view(b, 1)).view(b, self.N, 1, 1)
        mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
                   < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        
        # è·å–DCT Zig-Zagç´¢å¼•
        DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)
        
        # å®šä¹‰é‡‡æ ·æ“ä½œ
        def A_G(z):
            z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
            return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
        def A_DCT(z):
            dct_coeff = dct.dct_2d(z, norm='ortho')
            selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
            return selected * mask_DCT
        
        def AT_G(z):
            conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
            return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
        def AT_DCT(z):
            z_full = torch.zeros(b, 1, h, w, device=x.device)
            z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
            return dct.idct_2d(z_full, norm='ortho')
        
        A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
        AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
        return A, AT, mask_G, mask_DCT

    def forward(self, x, cs_ratio_batch):
        b, c, h, w = x.shape
        
        # è®¡ç®—åŒåˆ†æ”¯æµ‹é‡æ•° (é»˜è®¤æ¯”ä¾‹: Î³_D=0.4Î³, Î³_G=0.6Î³)
        total_m = int(cs_ratio_batch[0].item() * self.N)
        if self.mode == 'dct_only':
            q_G = torch.zeros(b, device=x.device).int()
            q_DCT = torch.full((b,), total_m, device=x.device).int()
        elif self.mode == 'gauss_only':
            q_G = torch.full((b,), total_m, device=x.device).int()
            q_DCT = torch.zeros(b, device=x.device).int()
        else: # dual mode (4:6 æ¯”ä¾‹)
            q_DCT = torch.tensor([int(total_m * 0.4)] * b, device=x.device).int()
            q_G = torch.tensor([total_m - int(total_m * 0.4)] * b, device=x.device).int()
        # q_G = torch.tensor([total_measurements * 0.6] * b, device=x.device).int()
        # q_DCT = torch.tensor([total_measurements * 0.4] * b, device=x.device).int()
        
        # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
        cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
        cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
        cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
        # æ·±åº¦æ¡ä»¶æ»¤æ³¢
        w_D, w_G = self.cond_filter(x, cs_ratio)
        # æ ¹æ®æ¨¡å¼å‡†å¤‡é‡‡æ ·è¾“å…¥ (ä¿æŒé€šé“é€»è¾‘ä¸€è‡´)
        x_D = x * w_D if self.mode != 'gauss_only' else torch.zeros_like(x)
        x_G = x * w_G if self.mode != 'dct_only' else torch.zeros_like(x)
        # x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
        # x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
        # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
        A, AT, mask_G, mask_DCT = self.define_sampling_operators(x, q_G, q_DCT)
        
        # åŒåˆ†æ”¯é‡‡æ ·
        x_filtered = torch.cat([x_G, x_D], dim=1)  # [B, 2, H, W]
        y = A(x_filtered)
        
        # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
        x_init_dual = AT(y)  # [B, 2, H, W]
        
        # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
        if self.mode == 'dual':
            x_init = torch.mean(x_init_dual, dim=1, keepdim=True)
        elif self.mode == 'dct_only':
            x_init = x_init_dual[:, 1:2, :, :] # å– DCT æ”¯è·¯ç»“æœ
        else:
            x_init = x_init_dual[:, 0:1, :, :] # å– Gauss æ”¯è·¯ç»“æœ
        # x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
        # é‡å»ºç½‘ç»œ
        z_pre = self.fe(x_init)  # [B, 31, H, W]
        z_cur = self.fe2(x_init) # [B, 31, H, W]
        x_recon = x_init         # [B, 1, H, W]
        for i in range(self.LayerNo):
            x_dual, z_next = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
            x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
            z_pre = z_cur
            z_cur = z_next
            
        return x_recon, y, A, q_G, q_DCT, (w_D, w_G)


# class DataConsistency(nn.Module):
#     def __init__(self):
#         super(DataConsistency, self).__init__()

#     def forward(self, x_recon, y_measured, mask):
#         """
#         x_recon: å½“å‰è¿­ä»£é‡å»ºçš„å›¾åƒ [B, 1, H, W]
#         y_measured: åŸå§‹é‡‡æ ·åˆ°çš„ K ç©ºé—´æµ‹é‡å€¼ (å¤æ•°å½¢å¼)
#         mask: é‡‡æ ·æ©ç  [B, 1, H, W]
#         """
#         # 1. å°†å›¾åƒè½¬åˆ°é¢‘åŸŸ (FFT)
#         # å‡è®¾ x_recon æ˜¯å®æ•°å›¾åƒï¼Œfft2 ä¼šè‡ªåŠ¨å¤„ç†
#         x_kspace = torch.fft.fft2(x_recon)
        
#         # 2. æ›¿æ¢é‡‡æ ·ç‚¹ï¼šåœ¨ mask ä¸º 1 çš„åœ°æ–¹ç”¨çœŸå® yï¼Œåœ¨ 0 çš„åœ°æ–¹ç”¨é¢„æµ‹å€¼
#         # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿ y_measured å’Œ x_kspace ç»´åº¦ä¸€è‡´
#         # å¦‚æœæ˜¯æ ‡å‡† MRIï¼ŒDC å…¬å¼å¦‚ä¸‹ï¼š
#         out_kspace = x_kspace + (y_measured - x_kspace) * mask
        
#         # 3. è½¬å›å›¾åƒåŸŸ (IFFT)
#         x_res = torch.fft.ifft2(out_kspace)
#         return torch.abs(x_res) # è¿”å›å®æ•°éƒ¨åˆ†ï¼ˆå¹…å€¼ï¼‰

# def to_3d(x):
#     return rearrange(x, 'b c h w -> b (h w) c')

# def to_4d(x,h,w):
#     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

# def zero_filled(x, mask, mod=False, norm=False):
#     x_dim_0 = x.shape[0]
#     x_dim_1 = x.shape[1]
#     x_dim_2 = x.shape[2]
#     x_dim_3 = x.shape[3]
#     x = x.view(-1, x_dim_2, x_dim_3, 1)

#     x_real = x
#     x_imag = torch.zeros_like(x_real)
#     x_complex = torch.cat([x_real, x_imag], 3)

#     x_kspace = torch.fft.fft2(x_complex)
#     y_kspace = x_kspace * mask
#     xu = torch.fft.ifft2(y_kspace)

#     if not mod:
#         xu_ret = xu[:, :, :, 0:1]
#     else:
#         xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

#     xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
#     xu_ret = xu_ret.float()

#     return xu_ret

# class BiasFree_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(BiasFree_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return x / torch.sqrt(sigma+1e-5) * self.weight

# class WithBias_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(WithBias_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         mu = x.mean(-1, keepdim=True)
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

# class LayerNorm(nn.Module):
#     def __init__(self, dim, LayerNorm_type):
#         super(LayerNorm, self).__init__()
#         if LayerNorm_type =='BiasFree':
#             self.body = BiasFree_LayerNorm(dim)
#         else:
#             self.body = WithBias_LayerNorm(dim)

#     def forward(self, x):
#         h, w = x.shape[-2:]
#         return to_4d(self.body(to_3d(x)), h, w)

# class BinaryQuantize(torch.autograd.Function):
#     @staticmethod
#     def forward(ctx, input, k, t):
#         ctx.save_for_backward(input, k, t)
#         out = torch.sigmoid(input * t)  
#         out = (out >= 0.5).float()
#         return out

#     @staticmethod
#     def backward(ctx, grad_output):
#         input, k, t = ctx.saved_tensors
#         grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
#         return grad_input, None, None, None

# class blockNL(torch.nn.Module):
#     def __init__(self, channels):
#         super(blockNL, self).__init__()
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
        
#         # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
#         self.norm_x = LayerNorm(32, 'WithBias')  # ä»1æ”¹ä¸º32
#         self.norm_z = LayerNorm(31, 'WithBias') 

#         self.t = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
#         self.p = nn.Sequential(
#             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
#         self.g1 = nn.Sequential(
#             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         self.g2 = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
#         )
#         self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
#         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»31+1=32æ”¹ä¸º31+32=63
#         self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
#         self.pos_emb = nn.Sequential(
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
#             nn.GELU(),
#             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
#         )
        
#         self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

#     def forward(self, x, z):
#         b, c, h, w = x.shape
#         x0 = self.norm_x(x)  
#         z0 = self.norm_z(z)  
#         z1 = self.t(z0)
#         b, c, h, w = z1.shape
#         z1 = z1.view(b, c, -1) 
#         x1 = self.p(x0)  
#         x1 = x1.view(b, c, -1) 
#         x2 = self.g1(x0)
#         x_v = x2.view(b, c, -1) 
#         z2 = self.g2(z0) 
#         z_v = z2.view(b, c, -1) 

#         num_heads = 4  
#         x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
#         z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
#         x_t_heads = x1_heads.permute(0, 1, 3, 2)  
#         att_heads = torch.matmul(z1_heads, x_t_heads) 
#         att_heads = self.softmax(att_heads)  

#         v_heads = self.w3*z_v_heads+self.w4*x_v_heads

#         out_x_heads = torch.matmul(att_heads, v_heads)  
#         out_x_heads = out_x_heads.view(b, c, h, w)  

#         out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
#         y = self.v(torch.cat([x, out_x_heads], 1))  # xæ˜¯32é€šé“ï¼Œout_x_headsæ˜¯31é€šé“ï¼Œæ‹¼æ¥åæ˜¯63é€šé“
#         return y

# class Atten(torch.nn.Module):
#     def __init__(self, channels):
#         super(Atten, self).__init__()
               
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
#         self.norm1 = LayerNorm(self.channels, 'WithBias')
#         self.norm2 = LayerNorm(self.channels, 'WithBias')
#         self.conv_qv1 = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_kv = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
#         self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
#     def forward(self, pre, cur, w1, w2):
#         b, c, h, w = pre.shape
#         pre_ln = self.norm1(pre)
#         cur_ln = self.norm2(cur)
#         q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
#         q = q.view(b, c, -1)  
#         v1 = v1.view(b, c, -1)
#         k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
#         k = k.view(b, c, -1)
#         v2 = v2.view(b, c, -1)
        
#         num_heads = 4  
#         q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         q = torch.nn.functional.normalize(q, dim=-1)
#         k = torch.nn.functional.normalize(k, dim=-1)
#         att = torch.matmul(q, k.permute(0, 1, 3, 2))  
#         att = self.softmax(att)
        
#         v = self.w1*v1+self.w2*v2
        
#         out = torch.matmul(att, v)  
#         out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
#         out = self.conv_out(out) + cur

#         return out

# class BasicBlock(torch.nn.Module):
#     def __init__(self):
#         super(BasicBlock, self).__init__()

#         self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
#         self.atten = Atten(31) 
#         self.nonlo = blockNL(channels=31) 
#         self.norm1 = LayerNorm(32, 'WithBias')
#         self.norm2 = LayerNorm(32, 'WithBias')
        
#         # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
#         self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
#         # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
#         self.grad_module = nn.Sequential(
#             nn.Conv2d(32, 32, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(32, 32, 3, padding=1)
#         )
        
#         self.conv_forward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )
#         self.conv_backward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )

#         # æ–°å¢ï¼šé¢‘åŸŸ DC å±‚
#         self.dc_layer = DataConsistency()
        
#         # æ–°å¢ï¼šé¢‘åŸŸç‰¹å¾ç»†åŒ– (å¯é€‰ï¼Œå¢åŠ åˆ›æ–°æ€§)
#         self.kspace_conv = nn.Sequential(
#             nn.Conv2d(2, 32, 1), # 2é€šé“å› ä¸º K ç©ºé—´æœ‰å®éƒ¨è™šéƒ¨
#             nn.ReLU(),
#             nn.Conv2d(32, 2, 1)
#         )
        
#         # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
#         self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
#     def forward(self, x, z_pre, z_cur, mask_G=None, y_G=None, mask_DCT=None, y_DCT=None, PhiTb=None, DCT_indices=None):    
#     # def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
#         # --- A. å›¾åƒåŸŸå¤„ç† (ä½ åŸæœ‰çš„é€»è¾‘) ---
#         x_expanded = self.channel_expand(x)     
#         z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
#         # --- ç¬¬äºŒæ­¥ï¼šé«˜æ–¯åˆ†æ”¯çš„åŒåŸŸä¿®æ­£ (åŸºäºæ¢¯åº¦çš„ DC) ---
#         # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
#         if PhiTb is not None:
#             # æ‰©å±•PhiTbçš„é€šé“ x = x + Î· * A_T(y - Ax)
#             PhiTb_expanded = self.channel_expand(PhiTb)
#             # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
#             x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
#         else:
#             x_grad = x_expanded
            
#         # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
#         x_grad_refined = self.grad_module(x_grad)
#         x_input = x_grad + x_grad_refined

#         # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
#         x_input = self.nonlo(x_input, z)

#         # æ®‹å·®å·ç§¯
#         x = self.norm1(x_input)
#         x_forward = self.conv_forward(x) + x_input
#         x = self.norm2(x_forward)
#         x_backward = self.conv_backward(x) + x_forward
#         x_pred_expanded = x_input + x_backward

#         # å‹ç¼©é€šé“: 32 -> 1
#         x_pred = self.channel_compress(x_pred_expanded)

#         # è·¯ B: æå–å‡ºè¾…åŠ©ç‰¹å¾ï¼Œä½œä¸ºä¸‹ä¸€è½®è¿­ä»£çš„ z_cur
#         # æˆ‘ä»¬å¯ä»¥å–å‰ 31 ä¸ªé€šé“ä½œä¸º zï¼Œæˆ–è€…å†åŠ ä¸€ä¸ªå·ç§¯æå– z
#         z_out = x_pred_expanded[:, :31, :, :] 

#          # --- B. é¢‘åŸŸå¤„ç† (è·¨åŸŸèåˆå…³é”®ç‚¹) ---
#         if y_DCT is not None and DCT_indices is not None:
#             # 1. è½¬åˆ° DCT åŸŸ
#             b, _, h, w = x_pred.shape
#             x_dct = dct.dct_2d(x_pred, norm='ortho')
#             # å°† y_DCT (æµ‹é‡å€¼) å¡«å›åˆ°å…¨å›¾ DCT ç³»æ•°ä¸­
#             # å‡è®¾ y_DCT å±•å¹³åçš„å½¢çŠ¶ä¸ç´¢å¼•åŒ¹é…
#             idx_x, idx_y = DCT_indices
            
#             # å…ˆå…‹éš†ä¸€ä»½ï¼Œé¿å…åŸåœ°æ“ä½œé”™è¯¯
#             x_dct_new = x_dct.clone()
#             # åªæ›¿æ¢é‡‡æ ·åˆ°çš„ç‚¹
#             # y_DCT éœ€è¦æ ¹æ®ä½ çš„ A_DCT é€»è¾‘ flatten
#             x_dct_new[:, :, idx_x, idx_y] = y_DCT.view(b, 1, -1) 
            
#             # åå˜æ¢å›å›¾åƒ
#             x_final = dct.idct_2d(x_dct_new, norm='ortho')
#         else:
#             x_final = x_pred

#         return x_final, z_out # è¿”å›ä¸¤ä¸ªå€¼ï¼

# # ä¸ä¼šè¢«æ‰‹åŠ¨è¦†ç›–
# # ---------------------------
# # æ¡ä»¶æ»¤æ³¢ (FiLM é£æ ¼)
# # ---------------------------
# class CondFilterV2(nn.Module):
#     def __init__(self, nf=16):
#         super().__init__()
#         self.nf = nf

#         # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
#         self.head = nn.Conv2d(1, nf//4, 3, padding=1)
#         self.body = nn.Sequential(
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1)
#         )
        
#         # CS ratio æ¡ä»¶ç¼©æ”¾
#         self.scale = nn.Sequential(
#             nn.Conv2d(2, nf//4, 1), 
#             nn.ReLU(), 
#             nn.Conv2d(nf//4, nf//4, 1)
#         )
        
#         self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

#     def forward(self, x, cs_ratio):
#         # å›¾åƒç‰¹å¾æå–
#         x_head = self.head(x)
        
#         # æ¡ä»¶ç¼©æ”¾
#         scaled = self.scale(cs_ratio) * self.body(x_head)
        
#         # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
#         weights = self.tail(scaled)
#         w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
#         return w_D, w_G

# def get_zigzag_ordered_indices(h=8, w=8, q=6):
#     x, y = [], []
#     x1, x2, y1, y2 = 0, 0, 0, 0
#     flag = True
#     while x2 < h or y1 < w:
#         if flag:
#             x = [*x, *range(x1, x2 - 1, -1)]
#             y = [*y, *range(y1, y2 + 1)]
#         else:
#             x = [*x, *range(x2, x1 + 1)]
#             y = [*y, *range(y2, y1 - 1, -1)]
#         flag = not flag
#         x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
#         x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
#     return x[:q], y[:q]

# def get_zigzag_truncated_indices(h=8, w=8, q=6):
#     if random.randint(0, 1):
#         x, y = get_zigzag_ordered_indices(h, w, q)
#     else:
#         y, x = get_zigzag_ordered_indices(w, h, q)
#     return x, y

# class COSO_LUCMT(nn.Module):
#     def __init__(self, LayerNo, B=32, nf=16):
#         super().__init__()
#         self.LayerNo = LayerNo
#         self.B = B
#         self.N = B * B
        
#         # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ
#         self.cond_filter = CondFilterV2(nf=nf)
        
#         # é«˜æ–¯åˆ†æ”¯æƒé‡ (å›ºå®š)
#         U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
#         self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        
#         # é‡å»ºç½‘ç»œ - ä¿®æ”¹ä¸ºå¤„ç†1é€šé“è¾“å…¥
#         self.fe = nn.Conv2d(1, 31, 3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
#         self.fe2 = nn.Conv2d(1, 31, 3, padding=1) # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
#         self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

#     def define_sampling_operators(self, x, q_G, q_DCT):
#         """å®šä¹‰é‡‡æ ·å’Œé‡å»ºæ“ä½œç¬¦ï¼Œç±»ä¼¼è®ºæ–‡ä¸­çš„Aå’ŒATå‡½æ•°"""
#         b, c, h, w = x.shape
#         n = h * w
#         h_B, w_B = h // self.B, w // self.B
        
#         # éšæœºåƒç´ ç½®ä¹±
#         perm = torch.randperm(n, device=x.device)
#         perm_inv = torch.empty_like(perm)
#         perm_inv[perm] = torch.arange(n, device=x.device)
        
#         # é«˜æ–¯åˆ†æ”¯éšæœºæƒé‡
#         A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        
#         # åˆ›å»ºæ©ç 
#         mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
#                  < q_G.view(b, 1)).view(b, self.N, 1, 1)
#         mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
#                    < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        
#         # è·å–DCT Zig-Zagç´¢å¼•
#         DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)

#         # æˆ‘ä»¬åªéœ€è¦å‰ q_DCT ä¸ªç´¢å¼•ä½œä¸ºæœ‰æ•ˆé‡‡æ ·ç‚¹
#         # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç›´æ¥è¿”å›å…¨é‡ç´¢å¼•ï¼ŒBlock å†…éƒ¨æ ¹æ® q_DCT æˆªå–
#         # æˆ–è€…ç›´æ¥è¿”å› truncated åçš„ç´¢å¼•
#         def get_mask_indices(b_idx):
#             q = q_DCT[b_idx].item()
#             return DCT_x[:q], DCT_y[:q]
        
#         # å®šä¹‰é‡‡æ ·æ“ä½œ
#         def A_G(z):
#             z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
#             return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
#         def A_DCT(z):
#             dct_coeff = dct.dct_2d(z, norm='ortho')
#             selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
#             return selected * mask_DCT
        
#         def AT_G(z):
#             conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
#             return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
#         def AT_DCT(z):
#             z_full = torch.zeros(b, 1, h, w, device=x.device)
#             z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
#             return dct.idct_2d(z_full, norm='ortho')
        
#         A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
#         AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
#         # ä½†ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬å°† DCT_x, DCT_y ä½œä¸º metadata è¿”å›
#         return A, AT, mask_G, mask_DCT, (DCT_x, DCT_y)

#     def forward(self, x, cs_ratio_batch):
#         b, c, h, w = x.shape
        
#         # è®¡ç®—åŒåˆ†æ”¯æµ‹é‡æ•° (é»˜è®¤æ¯”ä¾‹: Î³_D=0.4Î³, Î³_G=0.6Î³)
#         total_measurements = int(cs_ratio_batch[0].item() * self.N)
#         q_G = torch.tensor([total_measurements * 0.6] * b, device=x.device).int()
#         q_DCT = torch.tensor([total_measurements * 0.4] * b, device=x.device).int()
        
#         # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
#         cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
#         cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
#         cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
#         # æ·±åº¦æ¡ä»¶æ»¤æ³¢
#         w_D, w_G = self.cond_filter(x, cs_ratio)
#         x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
#         x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
#         # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
#         A, AT, mask_G, mask_DCT, DCT_info = self.define_sampling_operators(x, q_G, q_DCT)
        
#         # åŒåˆ†æ”¯é‡‡æ ·
#         x_filtered = torch.cat([x_D, x_G], dim=1)  # [B, 2, H, W]
#         y = A(x_filtered)# y[0] æ˜¯é«˜æ–¯æµ‹é‡å€¼, y[1] æ˜¯ DCT æµ‹é‡å€¼

#         # # ğŸš© åœ¨è¿™é‡Œæ’å…¥ç¬¬äºŒè¡Œï¼šæ£€æŸ¥é‡‡æ ·å€¼é‡çº§
#         # if getattr(self, 'print_once', True): # åªæ‰“å°ä¸€æ¬¡
#         #     print(f"y_DCT (é‡‡æ ·å€¼) èŒƒå›´: {y[1].min().item():.2f} to {y[1].max().item():.2f}")
#         #     self.print_once = False # æ‰“å°å®Œè®¾ä¸º False
        
#         # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
#         x_init_dual = AT(y)  # [B, 2, H, W]
        
#         # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
#         x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
#         # é‡å»ºç½‘ç»œ
#         z_pre = self.fe(x_init)  # [B, 31, H, W]
#         z_cur = self.fe2(x_init) # [B, 31, H, W]
#         x_recon = x_init         # [B, 1, H, W]
#         for i in range(self.LayerNo):
#             # x_dual = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
#             # x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
#             # z_pre = z_cur
#             # z_cur = x_dual[:, 1:, :, :] if x_dual.shape[1] > 1 else z_cur

#             x_recon, z_next = self.fcs[i](
#                 x_recon, 
#                 z_pre, 
#                 z_cur, 
#                 mask_G=mask_G,     # å¯¹åº” mask_G (é«˜æ–¯æ©ç )
#                 y_G=y[0],          # å¯¹åº” y_G (é«˜æ–¯æµ‹é‡å€¼)
#                 mask_DCT=mask_DCT, # é¢‘ç‡åŸŸæ©ç 
#                 y_DCT=y[1],        # é¢‘ç‡åŸŸçœŸç† (å¯¹åº”ç¬¬å››ä¸ªä¼˜åŒ–)
#                 PhiTb=x_init,       # ç©ºé—´åŸŸ/é«˜æ–¯è·¯å‚è€ƒ (å¯¹åº”é«˜æ–¯åˆ†æ”¯çš„ DC)
#                 DCT_indices=DCT_info # ä¼ å…¥ç´¢å¼•
#             )       
#             # æ›´æ–° z çš„æ»‘åŠ¨çª—å£
#             z_pre = z_cur
#             z_cur = z_next 
            
#         return x_recon

model = COSO_LUCMT(layer_num, mode='dct_only')
model = model.to(device)
# ä½¿ç”¨ DataParallel æ¥åˆ†é…åˆ°å¤šä¸ªGPU
# model = nn.DataParallel(model, device_ids=[0, 1]) 

print_flag = 1  

print("Training on device:", next(model.parameters()).device)


class RandomDataset(Dataset):
    def __init__(self, data, length):
        self.data = data
        self.len = length

    def __getitem__(self, index):
        return torch.Tensor(self.data[index, :]).float()

    def __len__(self):
        return self.len
dataset=RandomDataset(Training_labels, nrtrain)
# print(dataset[0])

if (platform.system() == "Windows"):
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=0,
                             shuffle=True)
else:
    rand_loader = DataLoader(dataset=RandomDataset(Training_labels, nrtrain), batch_size=batch_size, num_workers=2,
                             shuffle=True)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
# -------------------------
# æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜è·¯å¾„
# -------------------------
chosen_cs = 0.2
log_dir = os.path.join("log")
os.makedirs(log_dir, exist_ok=True)
# all:L1SSIMMeans
log_file_name = os.path.join(log_dir, f"Log_MRI_Dct_only_LUCMT_FastMRI-L1SSIMMeans_layer_{layer_num}_group_{group_num}_{chosen_cs}.txt")

model_dir = os.path.join("model", f"MRI_Dct_only_LUCMT_FastMRI-L1SSIMMeans_layer_{layer_num}_group_{group_num}_{chosen_cs}")
os.makedirs(model_dir, exist_ok=True)

if start_epoch > 0:
    pre_model_dir = model_dir
    model.load_state_dict(torch.load('%s/net_params_%d.pkl' % (pre_model_dir, start_epoch)))

# -------------------------
# CS ratio è®­ç»ƒæ¨¡å¼
# -------------------------
train_mode = "fixed"   # "random" è¡¨ç¤ºéšæœº CS ratioï¼Œ"fixed" è¡¨ç¤ºå›ºå®š
fixed_cs_ratios = [0.1, 0.2, 0.3, 0.4, 0.5]

# è®¾ç½®ç»˜å›¾åç«¯ï¼Œé˜²æ­¢åœ¨æ— ç•Œé¢çš„æœåŠ¡å™¨ä¸ŠæŠ¥é”™
plt.switch_backend('agg')

# åŠ è½½éªŒè¯é›†
Val_data = sio.loadmat(r'.\data\T1\val\fastMRI_val_T1_208.mat')
Val_labels = Val_data['reconstruction_esc'] # å‡è®¾ key æ˜¯è¿™ä¸ª
nrval = Val_labels.shape[0]

# éªŒè¯é›† DataLoader
val_loader = DataLoader(dataset=RandomDataset(Val_labels, nrval), 
                        batch_size=batch_size, shuffle=False, num_workers=0)

# ç”¨äºè®°å½•å†å²æ•°æ®
history = {
    'train_loss': [],
    'val_loss': [],
    'val_psnr': []
}

def plot_convergence(history, save_path):
    fig, ax1 = plt.subplots(figsize=(10, 6))

    # ç»˜åˆ¶ Loss æ›²çº¿ (å·¦è½´)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss', color='tab:red')
    ax1.plot(history['train_loss'], label='Train Loss', color='tab:red', linestyle='--')
    ax1.plot(history['val_loss'], label='Val Loss', color='tab:red')
    ax1.tick_params(axis='y', labelcolor='tab:red')
    ax1.grid(True, which='both', linestyle='--', alpha=0.5)

    # åˆ›å»ºå³è½´ç»˜åˆ¶ PSNR
    ax2 = ax1.twinx()
    ax2.set_ylabel('PSNR(dB)', color='tab:blue')
    ax2.plot(history['val_psnr'], label='Val PSNR', color='tab:blue', linewidth=2)
    ax2.tick_params(axis='y', labelcolor='tab:blue')

    fig.tight_layout()
    plt.title('Convergence Analysis (Loss and PSNR)')
    
    # åˆå¹¶å›¾ä¾‹
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

    plt.savefig(save_path)
    plt.close()

# -------------------------
# è®­ç»ƒå¾ªç¯
# -------------------------
for epoch_i in range(start_epoch + 1, end_epoch + 1):
    model.train()
    for data in rand_loader:
        # 1ï¸âƒ£ è¾“å…¥
        epoch_train_losses = []
        batch_x = data.to(device)
        # print(batch_x.shape)
        batch_x = batch_x.view(batch_x.shape[0], 1, batch_x.shape[1], batch_x.shape[2])
        batch_x, mean, std = transform.normalize_instance(batch_x, eps=1e-11)
        batch_x = batch_x.clamp(-6, 6)

        # # ğŸš© åœ¨è¿™é‡Œæ’å…¥ç¬¬ä¸€è¡Œï¼šæ£€æŸ¥å›¾åƒé‡çº§
        # print(f"--- æ•°å€¼æ£€æŸ¥ ---")
        # print(f"batch_x (å›¾åƒ) èŒƒå›´: {batch_x.min().item():.2f} to {batch_x.max().item():.2f}")

        cs_ratio_value = chosen_cs  # scalar

        # è½¬æˆ tensorï¼Œæ”¾åˆ° GPU
        cs_ratio_tensor = torch.tensor([[cs_ratio_value]], device=device).float()
        cs_ratio_batch = cs_ratio_tensor.expand(batch_x.shape[0], -1)  # [B,1]

        # 3ï¸âƒ£ forward
        # x_recon = model(batch_x, cs_ratio_batch)  # æ³¨æ„è¿™é‡Œä¼  tensor
        x_recon, y_true, A, q_G, q_DCT, (w_D, w_G) = model(batch_x, cs_ratio_batch)

        # loss_all = torch.mean(torch.pow(x_recon - batch_x, 2))
        # # 4ï¸âƒ£ Loss (å°è¯•ä½¿ç”¨ L1 Loss)
        # # loss_all = torch.nn.functional.l1_loss(x_recon, batch_x) 
        # # loss_all = torch.mean((x_recon - batch_x) ** 2)
        # # loss_all = torch.mean(torch.pow(x_recon - batch_x, 2))
        # # åœ¨è®­ç»ƒå¾ªç¯ä¸­
        l1_loss = torch.nn.functional.l1_loss(x_recon, batch_x)
        d_range = batch_x.max() - batch_x.min()
        # ssim è¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼Œæ‰€ä»¥ Loss ç”¨ 1 - ssim
        # non_negative=True ç¡®ä¿ ssim ä¸ºæ­£å€¼ï¼Œå¢åŠ ç¨³å®šæ€§
        ssim_val = ssim(x_recon, batch_x, data_range=d_range, size_average=True)
        loss_pixel = l1_loss + 0.5 * (1 - ssim_val)

        # 2. æµ‹é‡ä¸€è‡´æ€§ Loss (è¡¥å¿å»æ‰çš„åŒåŸŸå±‚)
        # 2. æµ‹é‡ä¸€è‡´æ€§ Loss (Measurement Fidelity) - è´Ÿè´£ç‰©ç†ä¿çœŸ
        # ã€å…³é”®ä¿®æ­£ã€‘ï¼šå¿…é¡»åº”ç”¨é‡‡æ ·æ—¶çš„æ¡ä»¶æƒé‡
        x_recon_D = x_recon * w_D # å¯¹é‡å»ºå›¾æ–½åŠ åŒæ ·çš„ DCT åˆ†æ”¯æƒé‡
        x_recon_G = x_recon * w_G # å¯¹é‡å»ºå›¾æ–½åŠ åŒæ ·çš„ Gaussian åˆ†æ”¯æƒé‡
        y_recon = A(torch.cat([x_recon_D, x_recon_G], dim=1)) # æ¨¡æ‹Ÿå½“æ—¶çš„ç‰©ç†é‡‡æ ·

        # y_true æ˜¯å‰å‘ä¼ æ’­æ—¶é‡‡æ ·å¾—åˆ°çš„çœŸå®æµ‹é‡å€¼
        loss_meas = torch.nn.functional.mse_loss(y_recon[0], y_true[0]) + \
                    torch.nn.functional.mse_loss(y_recon[1], y_true[1])

        # 3. é¢‘åŸŸæ„ŸçŸ¥æŸå¤±
        loss_dct = torch.nn.functional.l1_loss(dct.dct_2d(x_recon, norm='ortho'), 
                                                dct.dct_2d(batch_x, norm='ortho'))

        # æ€» Loss å¼•å¯¼
        loss_all = loss_pixel + 0.1 * loss_meas + 0.05 * loss_dct
        # loss_all = l1_loss

        # 5ï¸âƒ£ backward
        optimizer.zero_grad()
        loss_all.backward()
        optimizer.step()

        epoch_train_losses.append(loss_all.item())

        # 6ï¸âƒ£ æ‰“å°è®­ç»ƒä¿¡æ¯
        msg = (f"[{epoch_i:02d}/{end_epoch:02d}] "
               f"CS={cs_ratio_value:.2f} Loss={loss_all.item():.5f}")
        print(msg)

        with open(log_file_name, "a") as f:
            f.write(msg + "\n")

    # --- Validation Phase (æ¯ä¸ª Epoch ç»“æŸåè·‘ä¸€æ¬¡) ---
    model.eval()
    epoch_val_losses = []
    epoch_val_psnr = []
    
    with torch.no_grad():
        for data in val_loader:
            batch_x_val = data.to(device)
            if batch_x_val.dim() == 2: batch_x_val = batch_x_val.view(-1, 1, 256, 256)
            elif batch_x_val.dim() == 3: batch_x_val = batch_x_val.unsqueeze(1)

            batch_x_val, mean, std = transform.normalize_instance(batch_x_val, eps=1e-11)
            batch_x_val = batch_x_val.clamp(-6, 6)

            cs_ratio_v = torch.tensor([[chosen_cs]], device=device).expand(batch_x_val.shape[0], -1)
            
            x_res, _, _, _, _, _  = model(batch_x_val, cs_ratio_v)
            
            # è®¡ç®—éªŒè¯ Loss
            v_loss = torch.nn.functional.l1_loss(x_res, batch_x_val)
            epoch_val_losses.append(v_loss.item())
            
            # è®¡ç®—éªŒè¯ PSNR (åœ¨å½’ä¸€åŒ–åŸŸè®¡ç®—å³å¯ï¼Œç”¨äºè§‚å¯Ÿæ”¶æ•›æ€§)
            # æˆ–è€…åå½’ä¸€åŒ–è®¡ç®—æ›´å‡†ï¼Œè¿™é‡Œæ¨èåœ¨å½’ä¸€åŒ–åŸŸç®—ï¼Œé€Ÿåº¦å¿«
            for b in range(x_res.shape[0]):
                cur_psnr = evaluate.psnr(x_res[b,0].cpu().numpy(), batch_x_val[b,0].cpu().numpy())
                epoch_val_psnr.append(cur_psnr)

    # è®°å½•å¹¶ä¿å­˜å†å²
    history['train_loss'].append(np.mean(epoch_train_losses))
    history['val_loss'].append(np.mean(epoch_val_losses))
    history['val_psnr'].append(np.mean(epoch_val_psnr))

    # --- 2. åŠ¨æ€å‘½åæ–‡ä»¶ (å°† 0.4 æ›¿æ¢ä¸º {chosen_cs}) ---
    # ä½¿ç”¨ f-string è‡ªåŠ¨å¡«å……å½“å‰çš„é‡‡æ ·ç‡
    plot_filename = f"Convergence_Analysis_Dct_only_LUCMT_FastMRI-L1SSIMMeans-CS{chosen_cs}.png"
    mat_filename = f"training_history_Dct_only_LUCMT_FastMRI-L1SSIMMeans-CS{chosen_cs}.mat"

    # --- 3. æ‰§è¡Œç»˜å›¾ä¸ä¿å­˜ ---
    # è¿™æ ·å¦‚æœä½ è·‘ CS=0.1ï¼Œæ–‡ä»¶åå°±æ˜¯ ...-CS0.1.pngï¼›è·‘ 0.4 å°±æ˜¯ ...-CS0.4.png
    plot_convergence(history, os.path.join(log_dir, plot_filename))
    
    # åŒæ—¶ä¹Ÿä¿å­˜ä¸€ä»½æ•°æ®ï¼Œæ–¹ä¾¿ä»¥åç”¨ Origin æˆ– Excel é‡æ–°ç”»å›¾
    sio.savemat(os.path.join(log_dir, mat_filename), history)
    
    print(f"Epoch {epoch_i} Summary: Train Loss: {history['train_loss'][-1]:.4f}, "
            f"Val Loss: {history['val_loss'][-1]:.4f}, Val PSNR: {history['val_psnr'][-1]:.2f}")

    # 7ï¸âƒ£ ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), f"{model_dir}/net_params_{epoch_i}.pkl")

# # # åˆ†è¾¨ç‡æ— å…³çš„DISCOå·ç§¯å±‚
# # class DISCOConv2d(nn.Module):
# #     def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, 
# #                  radius=0.1, basis_functions=9, domain_range=(-1, 1)):
# #         super(DISCOConv2d, self).__init__()
# #         self.in_channels = in_channels
# #         self.out_channels = out_channels
# #         self.kernel_size = kernel_size
# #         self.stride = stride
# #         self.padding = padding
        
# #         # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
# #         self.radius = radius
# #         self.basis_functions = basis_functions
        
# #         # æ›´å¥½çš„æƒé‡åˆå§‹åŒ– - ä½¿ç”¨è¾ƒå°çš„åˆå§‹å€¼
# #         self.basis_weights = nn.Parameter(
# #             torch.randn(out_channels, in_channels, basis_functions) * 0.01
# #         )
        
# #         # åˆ›å»ºå›ºå®šçš„åŸºå‡½æ•°ç½‘æ ¼
# #         self.register_buffer('basis_grid', self.create_basis_grid())
        
# #         # åç½®é¡¹ - åˆå§‹åŒ–ä¸º0
# #         self.bias = nn.Parameter(torch.zeros(out_channels))
        
# #         # æ·»åŠ æ•°å€¼ç¨³å®šæ€§çš„å°å¸¸æ•°
# #         self.eps = 1e-8
        
# #     def create_basis_grid(self):
# #         """åˆ›å»ºåŸºå‡½æ•°ç½‘æ ¼"""
# #         grid_size = int(math.sqrt(self.basis_functions))
# #         basis_x = torch.linspace(-1, 1, grid_size)
# #         basis_y = torch.linspace(-1, 1, grid_size)
# #         basis_xx, basis_yy = torch.meshgrid(basis_x, basis_y, indexing='ij')
# #         basis_grid = torch.stack([basis_xx, basis_yy], dim=-1).reshape(-1, 2)
# #         return basis_grid
    
# #     def forward(self, x):
# #         """æ•°å€¼ç¨³å®šçš„å‰å‘ä¼ æ’­"""
# #         batch_size, channels, height_in, width_in = x.shape
        
# #         # æ£€æŸ¥è¾“å…¥
# #         if torch.isnan(x).any() or torch.isinf(x).any():
# #             print("DISCOå·ç§¯è¾“å…¥åŒ…å«NaNæˆ–Inf")
# #             return torch.zeros(batch_size, self.out_channels, height_in, width_in, device=x.device)
        
# #         # è®¡ç®—è¾“å‡ºå°ºå¯¸
# #         height_out = (height_in + 2 * self.padding - self.kernel_size) // self.stride + 1
# #         width_out = (width_in + 2 * self.padding - self.kernel_size) // self.stride + 1
        
# #         # å¯¹è¾“å…¥è¿›è¡Œå¡«å……
# #         if self.padding > 0:
# #             x_padded = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='reflect')
# #         else:
# #             x_padded = x
        
# #         # å±•å¼€è¾“å…¥ä¸ºå±€éƒ¨å—
# #         unfold = nn.Unfold(kernel_size=self.kernel_size, stride=self.stride)
# #         x_unfolded = unfold(x_padded)  # [B, C*kernel_size*kernel_size, H_out*W_out]
# #         x_unfolded = x_unfolded.view(batch_size, channels, self.kernel_size * self.kernel_size, -1)
        
# #         # ç”Ÿæˆæ ¸æƒé‡
# #         kernel_weights = self.generate_kernel_weights(x.device)
        
# #         # ä¿®å¤çŸ©é˜µä¹˜æ³•ç»´åº¦é—®é¢˜
# #         # x_unfolded: [B, C, K, N] å…¶ä¸­ K = kernel_size^2, N = H_out*W_out
# #         # kernel_weights: [1, O, C, K] å…¶ä¸­ O = out_channels
# #         # æˆ‘ä»¬éœ€è¦: [B, O, N]
        
# #         # ä½¿ç”¨einsumä¿®å¤ç»´åº¦é—®é¢˜
# #         output = torch.einsum('bcki,bock->boi', x_unfolded, kernel_weights)
        
# #         # æ·»åŠ åç½®å¹¶é‡å¡‘
# #         output = output + self.bias.unsqueeze(-1)
# #         output = output.reshape(batch_size, self.out_channels, height_out, width_out)
        
# #         # æ£€æŸ¥è¾“å‡º
# #         if torch.isnan(output).any() or torch.isinf(output).any():
# #             print("DISCOå·ç§¯è¾“å‡ºåŒ…å«NaNæˆ–Inf")
# #             output = torch.zeros_like(output)
            
# #         return output
    
# #     def generate_kernel_weights(self, device):
# #         """ç”Ÿæˆæ•°å€¼ç¨³å®šçš„æ ¸æƒé‡"""
# #         # åˆ›å»ºå±€éƒ¨é‚»åŸŸåæ ‡
# #         kernel_y = torch.linspace(-1, 1, self.kernel_size, device=device)
# #         kernel_x = torch.linspace(-1, 1, self.kernel_size, device=device)
# #         kernel_yy, kernel_xx = torch.meshgrid(kernel_y, kernel_x, indexing='ij')
# #         kernel_coords = torch.stack([kernel_xx, kernel_yy], dim=-1).reshape(-1, 2)  # [K, 2]
        
# #         # è®¡ç®—åŸºå‡½æ•°å€¼ - æ·»åŠ æ•°å€¼ç¨³å®šæ€§
# #         distances = torch.cdist(kernel_coords, self.basis_grid.to(device))
# #         basis_values = torch.exp(-distances.clamp(max=10)**2)  # [K, F]
        
# #         # åº”ç”¨å¯å­¦ä¹ æƒé‡ - ä¿®å¤ç»´åº¦
# #         # basis_values: [K, F]
# #         # self.basis_weights: [O, C, F]
# #         # æˆ‘ä»¬æƒ³è¦: [1, O, C, K]
        
# #         # ä½¿ç”¨einsumæ­£ç¡®è®¡ç®—
# #         kernel = torch.einsum('kf,ocf->ock', basis_values, self.basis_weights)
# #         kernel = kernel.unsqueeze(0)  # [1, O, C, K]
        
# #         return kernel.clamp(-5, 5)  # é™åˆ¶æ ¸æƒé‡èŒƒå›´


# # # æ›¿æ¢åŸæ¨¡å‹ä¸­çš„å·ç§¯å±‚ï¼Œä¿æŒå…¶ä»–ç»“æ„ä¸å˜

# # def to_3d(x):
# #     return rearrange(x, 'b c h w -> b (h w) c')

# # def to_4d(x, h, w):
# #     return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)

# # class BiasFree_LayerNorm(nn.Module):
# #     def __init__(self, normalized_shape):
# #         super(BiasFree_LayerNorm, self).__init__()
# #         if isinstance(normalized_shape, numbers.Integral):
# #             normalized_shape = (normalized_shape,)
# #         normalized_shape = torch.Size(normalized_shape)
# #         assert len(normalized_shape) == 1
# #         self.weight = nn.Parameter(torch.ones(normalized_shape))
# #         self.normalized_shape = normalized_shape

# #     def forward(self, x):
# #         sigma = x.var(-1, keepdim=True, unbiased=False)
# #         return x / torch.sqrt(sigma+1e-5) * self.weight

# # class WithBias_LayerNorm(nn.Module):
# #     def __init__(self, normalized_shape):
# #         super(WithBias_LayerNorm, self).__init__()
# #         if isinstance(normalized_shape, numbers.Integral):
# #             normalized_shape = (normalized_shape,)
# #         normalized_shape = torch.Size(normalized_shape)
# #         assert len(normalized_shape) == 1
# #         self.weight = nn.Parameter(torch.ones(normalized_shape))
# #         self.bias = nn.Parameter(torch.zeros(normalized_shape))
# #         self.normalized_shape = normalized_shape

# #     def forward(self, x):
# #         mu = x.mean(-1, keepdim=True)
# #         sigma = x.var(-1, keepdim=True, unbiased=False)
# #         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

# # class LayerNorm(nn.Module):
# #     def __init__(self, dim, LayerNorm_type):
# #         super(LayerNorm, self).__init__()
# #         if LayerNorm_type =='BiasFree':
# #             self.body = BiasFree_LayerNorm(dim)
# #         else:
# #             self.body = WithBias_LayerNorm(dim)

# #     def forward(self, x):
# #         h, w = x.shape[-2:]
# #         return to_4d(self.body(to_3d(x)), h, w)

# # class BinaryQuantize(torch.autograd.Function):
# #     @staticmethod
# #     def forward(ctx, input, k, t):
# #         ctx.save_for_backward(input, k, t)
# #         out = torch.sigmoid(input * t)  
# #         out = (out >= 0.5).float()
# #         return out

# #     @staticmethod
# #     def backward(ctx, grad_output):
# #         input, k, t = ctx.saved_tensors
# #         grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
# #         return grad_input, None, None, None

# # # å°†blockNLä¸­çš„å·ç§¯å±‚æ›¿æ¢ä¸ºDISCOå·ç§¯
# # class blockNL(torch.nn.Module):
# #     def __init__(self, channels):
# #         super(blockNL, self).__init__()
# #         self.channels = channels
# #         self.softmax = nn.Softmax(dim=-1)
        
# #         # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
# #         self.norm_x = LayerNorm(32, 'WithBias')
# #         self.norm_z = LayerNorm(31, 'WithBias')

# #         # å°†æ ‡å‡†å·ç§¯æ›¿æ¢ä¸ºDISCOå·ç§¯
# #         self.t = nn.Sequential(
# #             DISCOConv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1)
# #         )
        
# #         self.p = nn.Sequential(
# #             DISCOConv2d(in_channels=32, out_channels=self.channels, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1)
# #         )
        
# #         self.g1 = nn.Sequential(
# #             DISCOConv2d(in_channels=32, out_channels=self.channels, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1)
# #         )
        
# #         self.g2 = nn.Sequential(
# #             DISCOConv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1)
# #         )
        
# #         self.w = DISCOConv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, padding=0)
# #         self.v = DISCOConv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, padding=0)
        
# #         self.pos_emb = nn.Sequential(
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1),
# #             nn.GELU(),
# #             DISCOConv2d(self.channels, self.channels, kernel_size=3, padding=1),
# #         )
        
# #         self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
# #         self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

# #     def forward(self, x, z, w3, w4):
# #         b, c, h, w = x.shape
# #         x0 = self.norm_x(x)  
# #         z0 = self.norm_z(z)  
# #         z1 = self.t(z0)
# #         b, c, h, w = z1.shape
# #         z1 = z1.view(b, c, -1) 
# #         x1 = self.p(x0)  
# #         x1 = x1.view(b, c, -1) 
# #         x2 = self.g1(x0)
# #         x_v = x2.view(b, c, -1) 
# #         z2 = self.g2(z0) 
# #         z_v = z2.view(b, c, -1) 

# #         num_heads = 4  
# #         x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

# #         x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
# #         z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
# #         x_t_heads = x1_heads.permute(0, 1, 3, 2)  
# #         att_heads = torch.matmul(z1_heads, x_t_heads) 
# #         att_heads = self.softmax(att_heads)  

# #         v_heads = self.w3*z_v_heads+self.w4*x_v_heads

# #         out_x_heads = torch.matmul(att_heads, v_heads)  
# #         out_x_heads = out_x_heads.view(b, c, h, w)  

# #         out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
# #         y = self.v(torch.cat([x, out_x_heads], 1))
# #         return y

# # # å°†Attenä¸­çš„å·ç§¯å±‚æ›¿æ¢ä¸ºDISCOå·ç§¯
# # class Atten(torch.nn.Module):
# #     def __init__(self, channels):
# #         super(Atten, self).__init__()
               
# #         self.channels = channels
# #         self.softmax = nn.Softmax(dim=-1)
# #         self.norm1 = LayerNorm(self.channels, 'WithBias')
# #         self.norm2 = LayerNorm(self.channels, 'WithBias')
        
# #         # å°†æ ‡å‡†å·ç§¯æ›¿æ¢ä¸ºDISCOå·ç§¯
# #         self.conv_qv1 = nn.Sequential(
# #             DISCOConv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels*2, self.channels*2, kernel_size=3, padding=1)
# #         )
        
# #         self.conv_kv = nn.Sequential(
# #             DISCOConv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, padding=0),
# #             DISCOConv2d(self.channels*2, self.channels*2, kernel_size=3, padding=1)
# #         )
        
# #         self.conv_out = DISCOConv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, padding=0)
        
# #         self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
# #         self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
# #     def forward(self, pre, cur, w1, w2):
# #         b, c, h, w = pre.shape
# #         pre_ln = self.norm1(pre)
# #         cur_ln = self.norm2(cur)
# #         q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
# #         q = q.view(b, c, -1)  
# #         v1 = v1.view(b, c, -1)
# #         k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
# #         k = k.view(b, c, -1)
# #         v2 = v2.view(b, c, -1)
        
# #         num_heads = 4  
# #         q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

# #         q = torch.nn.functional.normalize(q, dim=-1)
# #         k = torch.nn.functional.normalize(k, dim=-1)
# #         att = torch.matmul(q, k.permute(0, 1, 3, 2))  
# #         att = self.softmax(att)
        
# #         v = self.w1*v1+self.w2*v2
        
# #         out = torch.matmul(att, v)  
# #         out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
# #         out = self.conv_out(out) + cur

# #         return out

# # # å°†BasicBlockä¸­çš„å·ç§¯å±‚æ›¿æ¢ä¸ºDISCOå·ç§¯
# # class BasicBlock(torch.nn.Module):
# #     def __init__(self):
# #         super(BasicBlock, self).__init__()

# #         self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
# #         self.atten = Atten(31) 
# #         self.nonlo = blockNL(channels=31) 
# #         self.norm1 = LayerNorm(32, 'WithBias')
# #         self.norm2 = LayerNorm(32, 'WithBias')
        
# #         # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“ (ä½¿ç”¨DISCOå·ç§¯)
# #         self.channel_expand = DISCOConv2d(1, 32, kernel_size=3, padding=1)
        
# #         # æ¢¯åº¦ä¸‹é™æ¨¡å— (ä½¿ç”¨DISCOå·ç§¯)
# #         self.grad_module = nn.Sequential(
# #             DISCOConv2d(32, 32, kernel_size=3, padding=1),
# #             nn.ReLU(),
# #             DISCOConv2d(32, 32, kernel_size=3, padding=1)
# #         )
        
# #         # ä½¿ç”¨DISCOå·ç§¯æ›¿æ¢æ ‡å‡†å·ç§¯
# #         self.conv_forward = nn.Sequential(
# #             DISCOConv2d(32, 32 * 4, kernel_size=1, padding=0),
# #             nn.GELU(),
# #             DISCOConv2d(32 * 4, 32 * 4, kernel_size=3, padding=1),
# #             nn.GELU(),
# #             DISCOConv2d(32 * 4, 32, kernel_size=1, padding=0),
# #         )
        
# #         self.conv_backward = nn.Sequential(
# #             DISCOConv2d(32, 32 * 4, kernel_size=1, padding=0),
# #             nn.GELU(),
# #             DISCOConv2d(32 * 4, 32 * 4, kernel_size=3, padding=1),
# #             nn.GELU(),
# #             DISCOConv2d(32 * 4, 32, kernel_size=1, padding=0),
# #         )
        
# #         # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“ (ä½¿ç”¨DISCOå·ç§¯)
# #         self.channel_compress = DISCOConv2d(32, 1, kernel_size=3, padding=1)
        
# #     def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
# #         # æ‰©å±•é€šé“: 1 -> 32
# #         x_expanded = self.channel_expand(x)
        
# #         z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
# #         # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
# #         if PhiTb is not None:
# #             # æ‰©å±•PhiTbçš„é€šé“
# #             PhiTb_expanded = self.channel_expand(PhiTb)
# #             # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
# #             x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
# #         else:
# #             x_grad = x_expanded
            
# #         # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
# #         x_grad_refined = self.grad_module(x_grad)
# #         x_input = x_grad + x_grad_refined

# #         # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
# #         x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

# #         # æ®‹å·®å·ç§¯
# #         x = self.norm1(x_input)
# #         x_forward = self.conv_forward(x) + x_input
# #         x = self.norm2(x_forward)
# #         x_backward = self.conv_backward(x) + x_forward
# #         x_pred_expanded = x_input + x_backward

# #         # å‹ç¼©é€šé“: 32 -> 1
# #         x_pred = self.channel_compress(x_pred_expanded)

# #         return x_pred

# # # å°†CondFilterä¸­çš„å·ç§¯å±‚æ›¿æ¢ä¸ºDISCOå·ç§¯
# # class CondFilterV2(nn.Module):
# #     def __init__(self, nf=16):
# #         super().__init__()
# #         self.nf = nf

# #         # ä½¿ç”¨DISCOå·ç§¯æ›¿æ¢æ ‡å‡†å·ç§¯
# #         self.head = DISCOConv2d(1, nf//4, kernel_size=3, padding=1)
# #         self.body = nn.Sequential(
# #             DISCOConv2d(nf//4, nf//4, kernel_size=3, padding=1),
# #             nn.ReLU(),
# #             DISCOConv2d(nf//4, nf//4, kernel_size=3, padding=1),
# #             nn.ReLU(),
# #             DISCOConv2d(nf//4, nf//4, kernel_size=3, padding=1)
# #         )
        
# #         # CS ratio æ¡ä»¶ç¼©æ”¾
# #         self.scale = nn.Sequential(
# #             DISCOConv2d(2, nf//4, kernel_size=1, padding=0),
# #             nn.ReLU(), 
# #             DISCOConv2d(nf//4, nf//4, kernel_size=1, padding=0)
# #         )
        
# #         self.tail = DISCOConv2d(nf//4, 2, kernel_size=3, padding=1)

# #     def forward(self, x, cs_ratio):
# #         # å›¾åƒç‰¹å¾æå–
# #         x_head = self.head(x)
        
# #         # æ¡ä»¶ç¼©æ”¾
# #         scaled = self.scale(cs_ratio) * self.body(x_head)
        
# #         # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
# #         weights = self.tail(scaled)
# #         w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
# #         return w_D, w_G

# # # æœ€ç»ˆæ¨¡å‹ - åªæ›¿æ¢å·ç§¯å±‚ä¸ºDISCOå·ç§¯ï¼Œä¿æŒå…¶ä»–ç»“æ„ä¸å˜
# # class COSO_LUCMT(nn.Module):
# #     def __init__(self, LayerNo, B=32, nf=16):
# #         super().__init__()
# #         self.LayerNo = LayerNo
# #         self.B = B
# #         self.N = B * B
        
# #         # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ (ä½¿ç”¨DISCOå·ç§¯)
# #         self.cond_filter = CondFilterV2(nf=nf)
        
# #         # é«˜æ–¯åˆ†æ”¯æƒé‡ (å›ºå®š)
# #         U, S, V = torch.linalg.svd(torch.randn(self.N, self.N))
# #         self.A_weight_G = nn.Parameter(U.mm(V).reshape(self.N, 1, B, B), requires_grad=False)
        
# #         # é‡å»ºç½‘ç»œ - ä½¿ç”¨DISCOå·ç§¯æ›¿æ¢æ ‡å‡†å·ç§¯
# #         self.fe = DISCOConv2d(1, 31, kernel_size=3, padding=1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
# #         self.fe2 = DISCOConv2d(1, 31, kernel_size=3, padding=1) # è¾“å…¥1é€šé“ï¼Œè¾“å‡º31é€šé“
# #         self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

# #     def define_sampling_operators(self, x, q_G, q_DCT):
# #         """å®šä¹‰é‡‡æ ·å’Œé‡å»ºæ“ä½œç¬¦ï¼Œç±»ä¼¼è®ºæ–‡ä¸­çš„Aå’ŒATå‡½æ•°"""
# #         b, c, h, w = x.shape
# #         n = h * w
# #         h_B, w_B = h // self.B, w // self.B
        
# #         # éšæœºåƒç´ ç½®ä¹±
# #         perm = torch.randperm(n, device=x.device)
# #         perm_inv = torch.empty_like(perm)
# #         perm_inv[perm] = torch.arange(n, device=x.device)
        
# #         # é«˜æ–¯åˆ†æ”¯éšæœºæƒé‡
# #         A_weight_G = self.A_weight_G[torch.randperm(self.N, device=x.device)].to(x.device)
        
# #         # åˆ›å»ºæ©ç 
# #         mask_G = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
# #                  < q_G.view(b, 1)).view(b, self.N, 1, 1)
# #         mask_DCT = (torch.arange(self.N, device=x.device).view(1, self.N).expand(b, self.N) 
# #                    < q_DCT.view(b, 1)).view(b, self.N, 1, 1)
        
# #         # è·å–DCT Zig-Zagç´¢å¼•
# #         DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, n)
        
# #         # å®šä¹‰é‡‡æ ·æ“ä½œ
# #         def A_G(z):
# #             z_perm = z.reshape(b, c, n)[:, :, perm].reshape(b, c, h, w)
# #             return F.conv2d(z_perm, A_weight_G, stride=self.B) * mask_G
        
# #         def A_DCT(z):
# #             dct_coeff = dct.dct_2d(z)
# #             selected = dct_coeff[:, :, DCT_x, DCT_y].reshape(b, self.N, h_B, w_B)
# #             return selected * mask_DCT
        
# #         def AT_G(z):
# #             conv_trans = F.conv_transpose2d(z, A_weight_G, stride=self.B)
# #             return conv_trans.reshape(b, c, n)[:, :, perm_inv].reshape(b, c, h, w)
        
# #         def AT_DCT(z):
# #             z_full = torch.zeros(b, 1, h, w, device=x.device)
# #             z_full[:, :, DCT_x, DCT_y] = z.reshape(b, 1, -1)
# #             return dct.idct_2d(z_full)
        
# #         A = lambda z: [A_G(z[:, 0:1]), A_DCT(z[:, 1:2])]
# #         AT = lambda z: torch.cat([AT_G(z[0]), AT_DCT(z[1])], dim=1)
        
# #         return A, AT, mask_G, mask_DCT

# #     def forward(self, x, cs_ratio_batch):
# #         b, c, h, w = x.shape
        
# #         # è®¡ç®—åŒåˆ†æ”¯æµ‹é‡æ•° (é»˜è®¤æ¯”ä¾‹: Î³_D=0.4Î³, Î³_G=0.6Î³)
# #         total_measurements = int(cs_ratio_batch[0].item() * self.N)
# #         q_G = torch.tensor([total_measurements * 0.6] * b, device=x.device).int()
# #         q_DCT = torch.tensor([total_measurements * 0.4] * b, device=x.device).int()
        
# #         # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
# #         cs_ratio_G = (q_G / self.N).view(b, 1, 1, 1)
# #         cs_ratio_DCT = (q_DCT / self.N).view(b, 1, 1, 1)
# #         cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
# #         # æ·±åº¦æ¡ä»¶æ»¤æ³¢
# #         w_D, w_G = self.cond_filter(x, cs_ratio)
# #         x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
# #         x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
# #         # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
# #         A, AT, mask_G, mask_DCT = self.define_sampling_operators(x, q_G, q_DCT)
        
# #         # åŒåˆ†æ”¯é‡‡æ ·
# #         x_filtered = torch.cat([x_D, x_G], dim=1)  # [B, 2, H, W]
# #         y = A(x_filtered)
        
# #         # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
# #         x_init_dual = AT(y)  # [B, 2, H, W]
        
# #         # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
# #         x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
# #         # é‡å»ºç½‘ç»œ
# #         z_pre = self.fe(x_init)  # [B, 31, H, W]
# #         z_cur = self.fe2(x_init) # [B, 31, H, W]
# #         x_recon = x_init         # [B, 1, H, W]
# #         for i in range(self.LayerNo):
# #             x_dual = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
# #             x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
# #             z_pre = z_cur
# #             z_cur = x_dual[:, 1:, :, :] if x_dual.shape[1] > 1 else z_cur
            
# #         return x_recon

# # # ä¿æŒåŸæœ‰çš„è¾…åŠ©å‡½æ•°ä¸å˜
# # def get_zigzag_ordered_indices(h=8, w=8, q=6):
# #     x, y = [], []
# #     x1, x2, y1, y2 = 0, 0, 0, 0
# #     flag = True
# #     while x2 < h or y1 < w:
# #         if flag:
# #             x = [*x, *range(x1, x2 - 1, -1)]
# #             y = [*y, *range(y1, y2 + 1)]
# #         else:
# #             x = [*x, *range(x2, x1 + 1)]
# #             y = [*y, *range(y2, y1 - 1, -1)]
# #         flag = not flag
# #         x1, y1 = (x1 + 1, 0) if (x1 < h - 1) else (h - 1, y1 + 1)
# #         x2, y2 = (0, y2 + 1) if (y2 < w - 1) else (x2 + 1, w - 1)
# #     return x[:q], y[:q]

# # def get_zigzag_truncated_indices(h=8, w=8, q=6):
# #     if random.randint(0, 1):
# #         x, y = get_zigzag_ordered_indices(h, w, q)
# #     else:
# #         y, x = get_zigzag_ordered_indices(w, h, q)
# #     return x, y

# # def zero_filled(x, mask, mod=False, norm=False):
# #     x_dim_0 = x.shape[0]
# #     x_dim_1 = x.shape[1]
# #     x_dim_2 = x.shape[2]
# #     x_dim_3 = x.shape[3]
# #     x = x.view(-1, x_dim_2, x_dim_3, 1)

# #     x_real = x
# #     x_imag = torch.zeros_like(x_real)
# #     x_complex = torch.cat([x_real, x_imag], 3)

# #     x_kspace = torch.fft.fft2(x_complex)
# #     y_kspace = x_kspace * mask
# #     xu = torch.fft.ifft2(y_kspace)

# #     if not mod:
# #         xu_ret = xu[:, :, :, 0:1]
# #     else:
# #         xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

# #     xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
# #     xu_ret = xu_ret.float()

# #     return xu_ret

# # def to_3d(x):
# #     return rearrange(x, 'b c h w -> b (h w) c')

# # def to_4d(x,h,w):
# #     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

# # def zero_filled(x, mask, mod=False, norm=False):
# #     x_dim_0 = x.shape[0]
# #     x_dim_1 = x.shape[1]
# #     x_dim_2 = x.shape[2]
# #     x_dim_3 = x.shape[3]
# #     x = x.view(-1, x_dim_2, x_dim_3, 1)

# #     x_real = x
# #     x_imag = torch.zeros_like(x_real)
# #     x_complex = torch.cat([x_real, x_imag], 3)

# #     x_kspace = torch.fft.fft2(x_complex)
# #     y_kspace = x_kspace * mask
# #     xu = torch.fft.ifft2(y_kspace)

# #     if not mod:
# #         xu_ret = xu[:, :, :, 0:1]
# #     else:
# #         xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

# #     xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
# #     xu_ret = xu_ret.float()

# #     return xu_ret

# # class BiasFree_LayerNorm(nn.Module):
# #     def __init__(self, normalized_shape):
# #         super(BiasFree_LayerNorm, self).__init__()
# #         if isinstance(normalized_shape, numbers.Integral):
# #             normalized_shape = (normalized_shape,)
# #         normalized_shape = torch.Size(normalized_shape)
# #         assert len(normalized_shape) == 1
# #         self.weight = nn.Parameter(torch.ones(normalized_shape))
# #         self.normalized_shape = normalized_shape

# #     def forward(self, x):
# #         sigma = x.var(-1, keepdim=True, unbiased=False)
# #         return x / torch.sqrt(sigma+1e-5) * self.weight

# # class WithBias_LayerNorm(nn.Module):
# #     def __init__(self, normalized_shape):
# #         super(WithBias_LayerNorm, self).__init__()
# #         if isinstance(normalized_shape, numbers.Integral):
# #             normalized_shape = (normalized_shape,)
# #         normalized_shape = torch.Size(normalized_shape)
# #         assert len(normalized_shape) == 1
# #         self.weight = nn.Parameter(torch.ones(normalized_shape))
# #         self.bias = nn.Parameter(torch.zeros(normalized_shape))
# #         self.normalized_shape = normalized_shape

# #     def forward(self, x):
# #         mu = x.mean(-1, keepdim=True)
# #         sigma = x.var(-1, keepdim=True, unbiased=False)
# #         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

# # class LayerNorm(nn.Module):
# #     def __init__(self, dim, LayerNorm_type):
# #         super(LayerNorm, self).__init__()
# #         if LayerNorm_type =='BiasFree':
# #             self.body = BiasFree_LayerNorm(dim)
# #         else:
# #             self.body = WithBias_LayerNorm(dim)

# #     def forward(self, x):
# #         h, w = x.shape[-2:]
# #         return to_4d(self.body(to_3d(x)), h, w)

# # class BinaryQuantize(torch.autograd.Function):
# #     @staticmethod
# #     def forward(ctx, input, k, t):
# #         ctx.save_for_backward(input, k, t)
# #         out = torch.sigmoid(input * t)  
# #         out = (out >= 0.5).float()
# #         return out

# #     @staticmethod
# #     def backward(ctx, grad_output):
# #         input, k, t = ctx.saved_tensors
# #         grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
# #         return grad_input, None, None, None

# # class blockNL(torch.nn.Module):
# #     def __init__(self, channels):
# #         super(blockNL, self).__init__()
# #         self.channels = channels
# #         self.softmax = nn.Softmax(dim=-1)
        
# #         # ä¿®æ”¹ä¸ºå¤„ç†32é€šé“è¾“å…¥
# #         self.norm_x = LayerNorm(32, 'WithBias')  # ä»1æ”¹ä¸º32
# #         self.norm_z = LayerNorm(31, 'WithBias') 

# #         self.t = nn.Sequential(
# #             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
# #         )
# #         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
# #         self.p = nn.Sequential(
# #             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
# #         )
# #         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»1æ”¹ä¸º32
# #         self.g1 = nn.Sequential(
# #             nn.Conv2d(in_channels=32, out_channels=self.channels, kernel_size=1, stride=1, bias=True),  # 32->31
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
# #         )
# #         self.g2 = nn.Sequential(
# #             nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True),
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels, bias=True)
# #         )
# #         self.w = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
# #         # ä¿®æ”¹è¾“å…¥é€šé“æ•°ä»31+1=32æ”¹ä¸º31+32=63
# #         self.v = nn.Conv2d(in_channels=self.channels+32, out_channels=32, kernel_size=1, stride=1, bias=True)  # 63->32
# #         self.pos_emb = nn.Sequential(
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
# #             nn.GELU(),
# #             nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, bias=False, groups=self.channels),
# #         )
        
# #         self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
# #         self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

# #     def forward(self, x, z, w3, w4):
# #         b, c, h, w = x.shape
# #         x0 = self.norm_x(x)  
# #         z0 = self.norm_z(z)  
# #         z1 = self.t(z0)
# #         b, c, h, w = z1.shape
# #         z1 = z1.view(b, c, -1) 
# #         x1 = self.p(x0)  
# #         x1 = x1.view(b, c, -1) 
# #         x2 = self.g1(x0)
# #         x_v = x2.view(b, c, -1) 
# #         z2 = self.g2(z0) 
# #         z_v = z2.view(b, c, -1) 

# #         num_heads = 4  
# #         x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

# #         x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
# #         z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
# #         x_t_heads = x1_heads.permute(0, 1, 3, 2)  
# #         att_heads = torch.matmul(z1_heads, x_t_heads) 
# #         att_heads = self.softmax(att_heads)  

# #         v_heads = self.w3*z_v_heads+self.w4*x_v_heads

# #         out_x_heads = torch.matmul(att_heads, v_heads)  
# #         out_x_heads = out_x_heads.view(b, c, h, w)  

# #         out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
# #         y = self.v(torch.cat([x, out_x_heads], 1))  # xæ˜¯32é€šé“ï¼Œout_x_headsæ˜¯31é€šé“ï¼Œæ‹¼æ¥åæ˜¯63é€šé“
# #         return y

# # class Atten(torch.nn.Module):
# #     def __init__(self, channels):
# #         super(Atten, self).__init__()
               
# #         self.channels = channels
# #         self.softmax = nn.Softmax(dim=-1)
# #         self.norm1 = LayerNorm(self.channels, 'WithBias')
# #         self.norm2 = LayerNorm(self.channels, 'WithBias')
# #         self.conv_qv1 = nn.Sequential(
# #             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
# #             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
# #         )
# #         self.conv_kv = nn.Sequential(
# #             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
# #             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
# #         )
# #         self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
# #         self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
# #         self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
# #     def forward(self, pre, cur, w1, w2):
# #         b, c, h, w = pre.shape
# #         pre_ln = self.norm1(pre)
# #         cur_ln = self.norm2(cur)
# #         q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
# #         q = q.view(b, c, -1)  
# #         v1 = v1.view(b, c, -1)
# #         k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
# #         k = k.view(b, c, -1)
# #         v2 = v2.view(b, c, -1)
        
# #         num_heads = 4  
# #         q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
# #         v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

# #         q = torch.nn.functional.normalize(q, dim=-1)
# #         k = torch.nn.functional.normalize(k, dim=-1)
# #         att = torch.matmul(q, k.permute(0, 1, 3, 2))  
# #         att = self.softmax(att)
        
# #         v = self.w1*v1+self.w2*v2
        
# #         out = torch.matmul(att, v)  
# #         out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
# #         out = self.conv_out(out) + cur

# #         return out

# # class BasicBlock(torch.nn.Module):
# #     def __init__(self):
# #         super(BasicBlock, self).__init__()

# #         self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
# #         self.atten = Atten(31) 
# #         self.nonlo = blockNL(channels=31) 
# #         self.norm1 = LayerNorm(32, 'WithBias')
# #         self.norm2 = LayerNorm(32, 'WithBias')
        
# #         # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
# #         self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
# #         # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
# #         self.grad_module = nn.Sequential(
# #             nn.Conv2d(32, 32, 3, padding=1),
# #             nn.ReLU(),
# #             nn.Conv2d(32, 32, 3, padding=1)
# #         )
        
# #         self.conv_forward = nn.Sequential(
# #             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
# #             nn.GELU(),
# #             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
# #             nn.GELU(),
# #             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
# #         )
# #         self.conv_backward = nn.Sequential(
# #             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
# #             nn.GELU(),
# #             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
# #             nn.GELU(),
# #             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
# #         )
        
# #         # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
# #         self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
# #     def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
# #         # æ‰©å±•é€šé“: 1 -> 32
# #         x_expanded = self.channel_expand(x)
        
# #         z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
# #         # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
# #         if PhiTb is not None:
# #             # æ‰©å±•PhiTbçš„é€šé“
# #             PhiTb_expanded = self.channel_expand(PhiTb)
# #             # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
# #             x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
# #         else:
# #             x_grad = x_expanded
            
# #         # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
# #         x_grad_refined = self.grad_module(x_grad)
# #         x_input = x_grad + x_grad_refined

# #         # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
# #         x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

# #         # æ®‹å·®å·ç§¯
# #         x = self.norm1(x_input)
# #         x_forward = self.conv_forward(x) + x_input
# #         x = self.norm2(x_forward)
# #         x_backward = self.conv_backward(x) + x_forward
# #         x_pred_expanded = x_input + x_backward

# #         # å‹ç¼©é€šé“: 32 -> 1
# #         x_pred = self.channel_compress(x_pred_expanded)

# #         return x_pred

# # # ä¸ä¼šè¢«æ‰‹åŠ¨è¦†ç›–
# # # ---------------------------
# # # æ¡ä»¶æ»¤æ³¢ (FiLM é£æ ¼)
# # # ---------------------------
# # class CondFilterV2(nn.Module):
# #     def __init__(self, nf=16):
# #         super().__init__()
# #         self.nf = nf

# #         # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
# #         self.head = nn.Conv2d(1, nf//4, 3, padding=1)
# #         self.body = nn.Sequential(
# #             nn.Conv2d(nf//4, nf//4, 3, padding=1),
# #             nn.ReLU(),
# #             nn.Conv2d(nf//4, nf//4, 3, padding=1),
# #             nn.ReLU(),
# #             nn.Conv2d(nf//4, nf//4, 3, padding=1)
# #         )
        
# #         # CS ratio æ¡ä»¶ç¼©æ”¾
# #         self.scale = nn.Sequential(
# #             nn.Conv2d(2, nf//4, 1), 
# #             nn.ReLU(), 
# #             nn.Conv2d(nf//4, nf//4, 1)
# #         )
        
# #         self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

# #     def forward(self, x, cs_ratio):
# #         # å›¾åƒç‰¹å¾æå–
# #         x_head = self.head(x)
        
# #         # æ¡ä»¶ç¼©æ”¾
# #         scaled = self.scale(cs_ratio) * self.body(x_head)
        
# #         # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
# #         weights = self.tail(scaled)
# #         w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
# #         return w_D, w_G

# # def get_zigzag_indices(h, w):
# #     """ç”Ÿæˆ zigzag é¡ºåºçš„ç´¢å¼•"""
# #     indices = []
# #     for sum_val in range(h + w - 1):
# #         if sum_val % 2 == 0:
# #             # å¶æ•°å¯¹è§’çº¿ï¼Œä»ä¸‹å¾€ä¸Š
# #             for i in range(min(sum_val, h-1), max(-1, sum_val-w), -1):
# #                 j = sum_val - i
# #                 if j < w:
# #                     indices.append((i, j))
# #         else:
# #             # å¥‡æ•°å¯¹è§’çº¿ï¼Œä»ä¸Šå¾€ä¸‹
# #             for i in range(max(0, sum_val-w+1), min(sum_val+1, h)):
# #                 j = sum_val - i
# #                 if j < w:
# #                     indices.append((i, j))
# #     return indices

# # def get_zigzag_truncated_indices(h, w, q):
# #     """è·å–å‰ q ä¸ª zigzag ç´¢å¼•"""
# #     indices = get_zigzag_indices(h, w)  # åªä¼  h, w ä¸¤ä¸ªå‚æ•°
# #     if q > len(indices):
# #         q = len(indices)
# #     x, y = zip(*indices[:q])
# #     return list(x), list(y)

# # class COSO_LUCMT(nn.Module):
# #     def __init__(self, LayerNo, nf=16):
# #         super().__init__()
# #         self.LayerNo = LayerNo
# #         # ç§»é™¤å›ºå®šçš„Bå’ŒNï¼Œæ”¹ä¸ºåŠ¨æ€è®¡ç®—
# #         self.cond_filter = CondFilterV2(nf=nf)
# #         self.fe = nn.Conv2d(1, 31, 3, padding=1)
# #         self.fe2 = nn.Conv2d(1, 31, 3, padding=1)
# #         self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])

# #     def define_sampling_operators(self, x, q_G, q_DCT):
# #         b, c, h, w = x.shape
# #         n = h * w  # åŠ¨æ€è®¡ç®—æ€»åƒç´ æ•°
        
# #         # å°†å¼ é‡è½¬æ¢ä¸ºæ•´æ•°
# #         q_G_val = q_G[0].item()  # è·å–ç¬¬ä¸€ä¸ªbatchçš„æµ‹é‡æ•°ï¼ˆå‡è®¾batchå†…ç›¸åŒï¼‰
# #         q_DCT_val = q_DCT[0].item()
        
# #         # ä½¿ç”¨çœŸæ­£çš„éšæœºé«˜æ–¯çŸ©é˜µ
# #         A_matrix_G = torch.randn(b, q_G_val, n, device=x.device) / math.sqrt(q_G_val)
        
# #         # æ­£ç¡®çš„DCTé‡‡æ · - åªå–å‰q_DCTä¸ªzigzagç³»æ•°
# #         DCT_x, DCT_y = get_zigzag_truncated_indices(h, w, q_DCT_val)
        
# #         def A(z):
# #             # åˆ†åˆ«å¤„ç†ä¸¤ä¸ªé€šé“
# #             z_G = z[:, 0:1]  # é«˜æ–¯åˆ†æ”¯ [b, 1, h, w]
# #             z_DCT = z[:, 1:2]  # DCTåˆ†æ”¯ [b, 1, h, w]
            
# #             # é«˜æ–¯åˆ†æ”¯é‡‡æ ·
# #             z_G_flat = z_G.view(b, -1)  # [b, n]
# #             y_G = torch.bmm(A_matrix_G, z_G_flat.unsqueeze(-1)).squeeze(-1)
# #             y_G = y_G.view(b, q_G_val, 1, 1)
            
# #             # DCTåˆ†æ”¯é‡‡æ ·
# #             dct_coeff = dct.dct_2d(z_DCT)
# #             selected = dct_coeff[:, :, DCT_x, DCT_y]
# #             y_DCT = selected.view(b, q_DCT_val, 1, 1)
            
# #             return [y_G, y_DCT]
        
# #         def AT(y):
# #             # é«˜æ–¯é‡å»º
# #             y_G_flat = y[0].view(b, q_G_val, 1)
# #             x_G_flat = torch.bmm(A_matrix_G.transpose(1, 2), y_G_flat)
# #             x_G = x_G_flat.view(b, 1, h, w)
            
# #             # DCTé‡å»º - ä¿®å¤ç»´åº¦é—®é¢˜
# #             y_DCT_flat = y[1].view(b, q_DCT_val)
            
# #             # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„DCTç³»æ•°çŸ©é˜µ
# #             dct_full = torch.zeros(b, 1, h, w, device=x.device)
            
# #             # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•æ–¹å¼
# #             for i in range(b):
# #                 # ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬èµ‹å€¼
# #                 dct_full[i, 0, DCT_x, DCT_y] = y_DCT_flat[i]
            
# #             x_DCT = dct.idct_2d(dct_full)
            
# #             return torch.cat([x_G, x_DCT], dim=1)
        
# #         return A, AT

# #     def forward(self, x, cs_ratio_batch):
# #         b, c, h, w = x.shape
# #         n = h * w  # åŠ¨æ€è®¡ç®—æ€»åƒç´ æ•°
        
# #         # æ­£ç¡®çš„é‡‡æ ·ç‡è®¡ç®—
# #         total_measurements = int(cs_ratio_batch[0].item() * n)
# #         q_G_val = int(total_measurements * 0.6)
# #         q_DCT_val = int(total_measurements * 0.4)
        
# #         # åˆ›å»ºå¼ é‡
# #         q_G = torch.tensor([q_G_val] * b, device=x.device)
# #         q_DCT = torch.tensor([q_DCT_val] * b, device=x.device)
        
# #         # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
# #         cs_ratio_G = (q_G / n).view(b, 1, 1, 1)
# #         cs_ratio_DCT = (q_DCT / n).view(b, 1, 1, 1)
# #         cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
# #         # æ·±åº¦æ¡ä»¶æ»¤æ³¢
# #         w_D, w_G = self.cond_filter(x, cs_ratio)
# #         x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
# #         x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
# #         # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦
# #         A, AT = self.define_sampling_operators(x, q_G, q_DCT)
        
# #         # åŒåˆ†æ”¯é‡‡æ ·
# #         x_filtered = torch.cat([x_D, x_G], dim=1)  # [B, 2, H, W]
# #         y = A(x_filtered)
        
# #         # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
# #         x_init_dual = AT(y)  # [B, 2, H, W]
        
# #         # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
# #         x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
# #         # é‡å»ºç½‘ç»œ
# #         z_pre = self.fe(x_init)  # [B, 31, H, W]
# #         z_cur = self.fe2(x_init) # [B, 31, H, W]
# #         x_recon = x_init         # [B, 1, H, W]
# #         for i in range(self.LayerNo):
# #             x_dual = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
# #             x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
# #             z_pre = z_cur
# #             z_cur = self.fe2(x_dual)
# #         return x_recon
# def to_3d(x):
#     return rearrange(x, 'b c h w -> b (h w) c')

# def to_4d(x,h,w):
#     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)

# def zero_filled(x, mask, mod=False, norm=False):
#     x_dim_0 = x.shape[0]
#     x_dim_1 = x.shape[1]
#     x_dim_2 = x.shape[2]
#     x_dim_3 = x.shape[3]
#     x = x.view(-1, x_dim_2, x_dim_3, 1)

#     x_real = x
#     x_imag = torch.zeros_like(x_real)
#     x_complex = torch.cat([x_real, x_imag], 3)

#     x_kspace = torch.fft.fft2(x_complex)
#     y_kspace = x_kspace * mask
#     xu = torch.fft.ifft2(y_kspace)

#     if not mod:
#         xu_ret = xu[:, :, :, 0:1]
#     else:
#         xu_ret = torch.sqrt(xu[..., 0:1] ** 2 + xu[..., 1:2] ** 2)

#     xu_ret = xu_ret.view(x_dim_0, x_dim_1, x_dim_2, x_dim_3)
#     xu_ret = xu_ret.float()

#     return xu_ret

# class BiasFree_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(BiasFree_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return x / torch.sqrt(sigma+1e-5) * self.weight

# class WithBias_LayerNorm(nn.Module):
#     def __init__(self, normalized_shape):
#         super(WithBias_LayerNorm, self).__init__()
#         if isinstance(normalized_shape, numbers.Integral):
#             normalized_shape = (normalized_shape,)
#         normalized_shape = torch.Size(normalized_shape)
#         assert len(normalized_shape) == 1
#         self.weight = nn.Parameter(torch.ones(normalized_shape))
#         self.bias = nn.Parameter(torch.zeros(normalized_shape))
#         self.normalized_shape = normalized_shape

#     def forward(self, x):
#         mu = x.mean(-1, keepdim=True)
#         sigma = x.var(-1, keepdim=True, unbiased=False)
#         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias

# class LayerNorm(nn.Module):
#     def __init__(self, dim, LayerNorm_type):
#         super(LayerNorm, self).__init__()
#         if LayerNorm_type =='BiasFree':
#             self.body = BiasFree_LayerNorm(dim)
#         else:
#             self.body = WithBias_LayerNorm(dim)

#     def forward(self, x):
#         h, w = x.shape[-2:]
#         return to_4d(self.body(to_3d(x)), h, w)

# class BinaryQuantize(torch.autograd.Function):
#     @staticmethod
#     def forward(ctx, input, k, t):
#         ctx.save_for_backward(input, k, t)
#         out = torch.sigmoid(input * t)  
#         out = (out >= 0.5).float()
#         return out

#     @staticmethod
#     def backward(ctx, grad_output):
#         input, k, t = ctx.saved_tensors
#         grad_input = k * t * (1 - torch.pow(torch.tanh(input * t * 2), 2)) * grad_output 
#         return grad_input, None, None, None

# class SerializableDISCOConv2d(nn.Module):
#     """
#     å®Œå…¨é‡å†™çš„å¯åºåˆ—åŒ–DISCOå·ç§¯ - é¿å…æ’å€¼é—®é¢˜
#     """
#     def __init__(self, in_channels, out_channels, radius_cutoff=0.02, 
#                  base_kernel_size=5, max_kernel_size=11, groups=1, bias=True):
#         super().__init__()
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.radius_cutoff = radius_cutoff
#         self.base_kernel_size = base_kernel_size
#         self.max_kernel_size = max_kernel_size
#         self.groups = groups
        
#         # é¢„å®šä¹‰å¤šä¸ªå›ºå®šå°ºå¯¸çš„å·ç§¯å±‚
#         self.kernel_sizes = [3, 5, 7, 9, 11]
#         self.conv_layers = nn.ModuleList([
#             nn.Conv2d(in_channels, out_channels, k, padding=k//2, groups=groups, bias=bias)
#             for k in self.kernel_sizes
#         ])
        
#         # æƒé‡å…±äº« - æ‰€æœ‰å·ç§¯å±‚ä½¿ç”¨ç›¸åŒçš„æƒé‡
#         self._init_weight_sharing()
    
#     def _init_weight_sharing(self):
#         """åˆå§‹åŒ–æƒé‡å…±äº«"""
#         # ä½¿ç”¨åŸºç¡€5x5å·ç§¯çš„æƒé‡ä½œä¸ºå‚è€ƒ
#         base_weight = self.conv_layers[1].weight.data  # ç´¢å¼•1å¯¹åº”5x5
        
#         # å°†æ‰€æœ‰å·ç§¯å±‚çš„æƒé‡è®¾ç½®ä¸ºç›¸åŒï¼ˆé€šè¿‡å¤åˆ¶å’Œæ’å€¼ï¼‰
#         for i, conv in enumerate(self.conv_layers):
#             if i == 1:  # 5x5ä¿æŒåŸæ ·
#                 continue
                
#             k = self.kernel_sizes[i]
#             if k < 5:
#                 # ä»5x5ä¸­å¿ƒè£å‰ª
#                 start = (5 - k) // 2
#                 conv.weight.data = base_weight[:, :, start:start+k, start:start+k]
#             else:
#                 # ä»5x5æ’å€¼æ‰©å±•
#                 with torch.no_grad():
#                     expanded_weight = F.interpolate(
#                         base_weight,
#                         size=(k, k),
#                         mode='bilinear',
#                         align_corners=False
#                     )
#                     conv.weight.data = expanded_weight
    
#     def _get_target_kernel_size(self, h, w):
#         """æ ¹æ®è¾“å…¥åˆ†è¾¨ç‡è®¡ç®—ç›®æ ‡æ ¸å¤§å°"""
#         target_size = max(3, int(min(h, w) * self.radius_cutoff))
#         if target_size % 2 == 0:
#             target_size += 1
#         return min(target_size, self.max_kernel_size)
    
#     def _find_closest_kernel_size(self, target_size):
#         """æ‰¾åˆ°é¢„å®šä¹‰æ ¸å¤§å°ä¸­æœ€æ¥è¿‘çš„ä¸€ä¸ª"""
#         return min(self.kernel_sizes, key=lambda x: abs(x - target_size))
    
#     def forward(self, x):
#         b, c, h, w = x.shape
        
#         # è®¡ç®—ç›®æ ‡æ ¸å¤§å°
#         target_size = self._get_target_kernel_size(h, w)
        
#         # æ‰¾åˆ°æœ€æ¥è¿‘çš„é¢„å®šä¹‰æ ¸å¤§å°
#         closest_size = self._find_closest_kernel_size(target_size)
        
#         # ä½¿ç”¨å¯¹åº”çš„å·ç§¯å±‚
#         size_index = self.kernel_sizes.index(closest_size)
#         output = self.conv_layers[size_index](x)
        
#         return output
    
#     def clear_cache(self):
#         """æ¸…ç©ºç¼“å­˜ï¼ˆä¿æŒæ¥å£ä¸€è‡´ï¼‰"""
#         pass

# class SerializableNeuralOperatorBlock(nn.Module):
#     """
#     å¯åºåˆ—åŒ–çš„ç¥ç»ç®—å­æ¨¡å—
#     """
#     def __init__(self, in_channels, out_channels, hidden_channels=32, 
#                  radius_cutoff=0.02, num_layers=2):
#         super().__init__()
        
#         layers = []
#         current_in = in_channels
        
#         for i in range(num_layers):
#             current_out = hidden_channels if i < num_layers - 1 else out_channels
#             layers.extend([
#                 SerializableDISCOConv2d(current_in, current_out, radius_cutoff),
#                 nn.InstanceNorm2d(current_out),
#                 nn.LeakyReLU(0.2, inplace=True) if i < num_layers - 1 else nn.Identity()
#             ])
#             current_in = current_out
            
#         self.net = nn.Sequential(*layers)
        
#     def forward(self, x):
#         return self.net(x)
    
#     def clear_cache(self):
#         """æ¸…ç©ºæ‰€æœ‰DISCOConv2dçš„ç¼“å­˜"""
#         for module in self.net:
#             if hasattr(module, 'clear_cache'):
#                 module.clear_cache()

# class BlockNL(torch.nn.Module):
#     def __init__(self, channels, radius_cutoff=0.02):
#         super(BlockNL, self).__init__()
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
        
#         self.norm_x = LayerNorm(32, 'WithBias')
#         self.norm_z = LayerNorm(31, 'WithBias')

#         # ä½¿ç”¨å¯åºåˆ—åŒ–çš„ç¥ç»ç®—å­
#         self.t = SerializableNeuralOperatorBlock(31, channels, radius_cutoff=radius_cutoff)
#         self.p = SerializableNeuralOperatorBlock(32, channels, radius_cutoff=radius_cutoff)
#         self.g1 = SerializableNeuralOperatorBlock(32, channels, radius_cutoff=radius_cutoff)
#         self.g2 = SerializableNeuralOperatorBlock(31, channels, radius_cutoff=radius_cutoff)
        
#         # æŠ•å½±å±‚ä½¿ç”¨å¯åºåˆ—åŒ–ç‰ˆæœ¬
#         self.w = SerializableDISCOConv2d(channels, channels, radius_cutoff=0.001)
#         self.v = nn.Conv2d(channels + 32, 32, 1, bias=True)  # 1x1å·ç§¯ä¿æŒ
        
#         self.pos_emb = SerializableNeuralOperatorBlock(channels, channels, radius_cutoff=radius_cutoff)
        
#         self.w3 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w4 = nn.Parameter(torch.randn(1, requires_grad=True))

#     def forward(self, x, z, w3, w4):
#         # ä¿æŒåŸæœ‰forwardé€»è¾‘å®Œå…¨ä¸å˜ï¼
#         b, c, h, w = x.shape
#         x0 = self.norm_x(x)  
#         z0 = self.norm_z(z)  
        
#         z1 = self.t(z0)
#         b, c, h, w = z1.shape
#         z1 = z1.view(b, c, -1) 
#         x1 = self.p(x0)  
#         x1 = x1.view(b, c, -1) 
#         x2 = self.g1(x0)
#         x_v = x2.view(b, c, -1) 
#         z2 = self.g2(z0) 
#         z_v = z2.view(b, c, -1) 

#         num_heads = 4  
#         x1_heads = x1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z1_heads = z1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         z_v_heads = z_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         x_v_heads = x_v.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         x1_heads = torch.nn.functional.normalize(x1_heads, dim=-1)
#         z1_heads = torch.nn.functional.normalize(z1_heads, dim=-1)
#         x_t_heads = x1_heads.permute(0, 1, 3, 2)  
#         att_heads = torch.matmul(z1_heads, x_t_heads) 
#         att_heads = self.softmax(att_heads)  

#         v_heads = self.w3 * z_v_heads + self.w4 * x_v_heads
#         out_x_heads = torch.matmul(att_heads, v_heads)  
#         out_x_heads = out_x_heads.view(b, c, h, w)  

#         out_x_heads = self.w(out_x_heads) + self.pos_emb(z2) + z  
#         y = self.v(torch.cat([x, out_x_heads], 1))
        
#         return y
    
#     def clear_cache(self):
#         """æ¸…ç©ºæ‰€æœ‰å­æ¨¡å—çš„ç¼“å­˜"""
#         self.t.clear_cache()
#         self.p.clear_cache()
#         self.g1.clear_cache()
#         self.g2.clear_cache()
#         self.w.clear_cache()
#         self.pos_emb.clear_cache()
    
# class Atten(torch.nn.Module):
#     def __init__(self, channels):
#         super(Atten, self).__init__()
               
#         self.channels = channels
#         self.softmax = nn.Softmax(dim=-1)
#         self.norm1 = LayerNorm(self.channels, 'WithBias')
#         self.norm2 = LayerNorm(self.channels, 'WithBias')
#         self.conv_qv1 = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_kv = nn.Sequential(
#             nn.Conv2d(in_channels=self.channels, out_channels=self.channels*2, kernel_size=1, stride=1, bias=True),
#             nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, groups=self.channels*2, bias=True)
#         )
#         self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=1, stride=1, bias=True)
        
#         self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
#         self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))
    
#     def forward(self, pre, cur, w1, w2):
#         b, c, h, w = pre.shape
#         pre_ln = self.norm1(pre)
#         cur_ln = self.norm2(cur)
#         q,v1 = self.conv_qv1(cur_ln).chunk(2, dim=1)
#         q = q.view(b, c, -1)  
#         v1 = v1.view(b, c, -1)
#         k, v2 = self.conv_kv(pre_ln).chunk(2, dim=1)  
#         k = k.view(b, c, -1)
#         v2 = v2.view(b, c, -1)
        
#         num_heads = 4  
#         q = q.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         k = k.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v1 = v1.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  
#         v2 = v2.view(b, c, num_heads, -1).permute(0, 2, 1, 3)  

#         q = torch.nn.functional.normalize(q, dim=-1)
#         k = torch.nn.functional.normalize(k, dim=-1)
#         att = torch.matmul(q, k.permute(0, 1, 3, 2))  
#         att = self.softmax(att)
        
#         v = self.w1*v1+self.w2*v2
        
#         out = torch.matmul(att, v)  
#         out = out.permute(0, 2, 1, 3).contiguous().view(b, c, h, w)  
#         out = self.conv_out(out) + cur

#         return out

# class BasicBlock(torch.nn.Module):
#     def __init__(self):
#         super(BasicBlock, self).__init__()

#         self.lambda_step = nn.Parameter(torch.Tensor([0.5]))
#         self.atten = Atten(31) 
#         self.nonlo = BlockNL(channels=31) 
#         self.norm1 = LayerNorm(32, 'WithBias')
#         self.norm2 = LayerNorm(32, 'WithBias')
        
#         # é€šé“æ‰©å±•å±‚ - å°†1é€šé“æ‰©å±•åˆ°32é€šé“
#         self.channel_expand = nn.Conv2d(1, 32, 3, padding=1)
        
#         # æ¢¯åº¦ä¸‹é™æ¨¡å— (å¯¹åº”è®ºæ–‡ä¸­çš„æ¢¯åº¦è®¡ç®—)
#         self.grad_module = nn.Sequential(
#             nn.Conv2d(32, 32, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(32, 32, 3, padding=1)
#         )
        
#         self.conv_forward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )
#         self.conv_backward = nn.Sequential(
#             nn.Conv2d(32, 32 * 4, 1, 1, bias=False),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32 * 4, 3, 1, 1, bias=False, groups=32 * 4),
#             nn.GELU(),
#             nn.Conv2d(32 * 4, 32, 1, 1, bias=False),
#         )
        
#         # é€šé“å‹ç¼©å±‚ - å°†32é€šé“å‹ç¼©å›1é€šé“
#         self.channel_compress = nn.Conv2d(32, 1, 3, padding=1)
        
#     def forward(self, x, z_pre, z_cur, mask=None, PhiTb=None):
#         # æ‰©å±•é€šé“: 1 -> 32
#         x_expanded = self.channel_expand(x)
        
#         z = self.atten(z_pre, z_cur, w1=1.0, w2=1.0)
        
#         # æ”¹è¿›çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤
#         if PhiTb is not None:
#             # æ‰©å±•PhiTbçš„é€šé“
#             PhiTb_expanded = self.channel_expand(PhiTb)
#             # æ¢¯åº¦ä¸‹é™: x - Î· * gradient
#             x_grad = x_expanded + self.lambda_step * (PhiTb_expanded - x_expanded)
#         else:
#             x_grad = x_expanded
            
#         # è¿›ä¸€æ­¥ç”¨å·ç§¯ç»†åŒ–æ¢¯åº¦æ­¥éª¤
#         x_grad_refined = self.grad_module(x_grad)
#         x_input = x_grad + x_grad_refined

#         # éçº¿æ€§å— (è¿‘ç«¯æ˜ å°„)
#         x_input = self.nonlo(x_input, z, w3=1.0, w4=1.0)

#         # æ®‹å·®å·ç§¯
#         x = self.norm1(x_input)
#         x_forward = self.conv_forward(x) + x_input
#         x = self.norm2(x_forward)
#         x_backward = self.conv_backward(x) + x_forward
#         x_pred_expanded = x_input + x_backward

#         # å‹ç¼©é€šé“: 32 -> 1
#         x_pred = self.channel_compress(x_pred_expanded)

#         return x_pred

# class CondFilterV2(nn.Module):
#     def __init__(self, nf=16):
#         super().__init__()
#         self.nf = nf

#         # ç±»ä¼¼è®ºæ–‡ä¸­SSæ¨¡å—çš„ç»“æ„
#         self.head = nn.Conv2d(1, nf//4, 3, padding=1)
#         self.body = nn.Sequential(
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1),
#             nn.ReLU(),
#             nn.Conv2d(nf//4, nf//4, 3, padding=1)
#         )
        
#         # CS ratio æ¡ä»¶ç¼©æ”¾
#         self.scale = nn.Sequential(
#             nn.Conv2d(2, nf//4, 1), 
#             nn.ReLU(), 
#             nn.Conv2d(nf//4, nf//4, 1)
#         )
        
#         self.tail = nn.Conv2d(nf//4, 2, 3, padding=1)

#     def forward(self, x, cs_ratio):
#         # å›¾åƒç‰¹å¾æå–
#         x_head = self.head(x)
        
#         # æ¡ä»¶ç¼©æ”¾
#         scaled = self.scale(cs_ratio) * self.body(x_head)
        
#         # è¾“å‡ºä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
#         weights = self.tail(scaled)
#         w_D, w_G = weights[:, 0:1], weights[:, 1:2]
        
#         return w_D, w_G

# def get_zigzag_indices(h, w):
#     """ç”Ÿæˆ zigzag é¡ºåºçš„ç´¢å¼• - åˆ†è¾¨ç‡æ— å…³ç‰ˆæœ¬"""
#     indices = []
#     for sum_val in range(h + w - 1):
#         if sum_val % 2 == 0:
#             # å¶æ•°å¯¹è§’çº¿ï¼Œä»ä¸‹å¾€ä¸Š
#             for i in range(min(sum_val, h-1), max(-1, sum_val-w), -1):
#                 j = sum_val - i
#                 if j < w:
#                     indices.append((i, j))
#         else:
#             # å¥‡æ•°å¯¹è§’çº¿ï¼Œä»ä¸Šå¾€ä¸‹
#             for i in range(max(0, sum_val-w+1), min(sum_val+1, h)):
#                 j = sum_val - i
#                 if j < w:
#                     indices.append((i, j))
#     return indices

# def get_zigzag_truncated_indices(h, w, q):
#     """è·å–å‰ q ä¸ª zigzag ç´¢å¼• - åˆ†è¾¨ç‡æ— å…³ç‰ˆæœ¬"""
#     indices = get_zigzag_indices(h, w)
#     if q > len(indices):
#         q = len(indices)
#     x, y = zip(*indices[:q])
#     return list(x), list(y)

# def dct_2d(x):
#     """ä¼˜åŒ–çš„2D DCTå®ç° - é¿å…å¤§çŸ©é˜µè¿ç®—"""
#     b, c, h, w = x.shape
#     device = x.device
    
#     # ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„é€è¡ŒDCT
#     result = torch.zeros_like(x)
    
#     for i in range(b):
#         for j in range(c):
#             # å¯¹æ¯ä¸ªé€šé“å•ç‹¬å¤„ç†ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
#             channel_data = x[i, j]
            
#             # ä½¿ç”¨FFTå®ç°DCTæ¥èŠ‚çœå†…å­˜
#             n = h * w
#             x_expanded = torch.cat([channel_data, torch.flip(channel_data, [0, 1])], dim=0)
#             x_expanded = torch.cat([x_expanded, torch.flip(x_expanded, [0])], dim=1)
            
#             # ä½¿ç”¨FFT
#             X = torch.fft.fft2(x_expanded)
            
#             # æå–DCTç³»æ•°
#             dct_coeff = X.real[:h, :w]
#             result[i, j] = dct_coeff
    
#     return result

# def idct_2d(x):
#     """ä¼˜åŒ–çš„2Dé€†DCTå®ç°"""
#     b, c, h, w = x.shape
#     device = x.device
    
#     result = torch.zeros_like(x)
    
#     for i in range(b):
#         for j in range(c):
#             channel_data = x[i, j]
            
#             # æ„å»ºå¯¹ç§°æ‰©å±•
#             x_expanded = torch.zeros(2*h, 2*w, device=device)
#             x_expanded[:h, :w] = channel_data
#             x_expanded[:h, w:] = torch.flip(channel_data, [1])
#             x_expanded[h:, :] = torch.flip(x_expanded[:h, :], [0])
            
#             # ä½¿ç”¨FFT
#             X = torch.fft.ifft2(x_expanded)
            
#             # æå–ç»“æœ
#             result[i, j] = X.real[:h, :w] * 4  # ç¼©æ”¾å› å­
    
#     return result

# def generate_deterministic_gaussian_matrix(rows, cols, resolution_key, device):
#     """åŸºäºåˆ†è¾¨ç‡ç”Ÿæˆç¡®å®šæ€§çš„é«˜æ–¯çŸ©é˜µ"""
#     # åˆ›å»ºåŸºäºåˆ†è¾¨ç‡çš„ç¡®å®šæ€§ç§å­
#     seed_str = f"gaussian_{resolution_key}"
#     seed = int(hashlib.md5(seed_str.encode()).hexdigest()[:8], 16) % (2**31)
    
#     # ä¿å­˜å½“å‰éšæœºçŠ¶æ€
#     original_rng_state = torch.get_rng_state()
#     torch.manual_seed(seed)
    
#     try:
#         # ç”Ÿæˆé«˜æ–¯çŸ©é˜µ
#         matrix = torch.randn(rows, cols, device=device) / math.sqrt(rows)
#         return matrix
#     finally:
#         # æ¢å¤éšæœºçŠ¶æ€
#         torch.set_rng_state(original_rng_state)

# class ResolutionAwareSamplingSystem:
#     """
#     åˆ†è¾¨ç‡æ„ŸçŸ¥çš„é‡‡æ ·ç³»ç»Ÿ - ä½¿ç”¨æ–¹æ³•ä¸€çš„é‡‡æ ·ç­–ç•¥
#     """
#     def __init__(self, device):
#         self.device = device
#         self.dct_indices_cache = {}  # ç¼“å­˜DCTç´¢å¼•
#         self.gaussian_matrices_cache = {}  # ç¼“å­˜é«˜æ–¯çŸ©é˜µ
    
#     def get_dct_indices(self, h, w, q_DCT):
#         """è·å–DCTé‡‡æ ·ç´¢å¼•"""
#         key = f"{h}_{w}_{q_DCT}"
#         if key not in self.dct_indices_cache:
#             self.dct_indices_cache[key] = get_zigzag_truncated_indices(h, w, q_DCT)
#         return self.dct_indices_cache[key]
    
#     def get_gaussian_matrix(self, h, w, q_G, q_DCT, batch_size):
#         """è·å–é«˜æ–¯çŸ©é˜µ - åŸºäºåˆ†è¾¨ç‡ç”Ÿæˆç¡®å®šæ€§çŸ©é˜µ"""
#         resolution_key = f"{h}_{w}"
#         matrix_key = f"{resolution_key}_{q_G}"
        
#         if matrix_key not in self.gaussian_matrices_cache:
#             n = h * w
#             # ç”Ÿæˆç¡®å®šæ€§çš„é«˜æ–¯çŸ©é˜µ
#             matrix = generate_deterministic_gaussian_matrix(q_G, n, resolution_key, self.device)
#             # æ‰©å±•åˆ°batchç»´åº¦
#             self.gaussian_matrices_cache[matrix_key] = matrix.unsqueeze(0).expand(batch_size, -1, -1)
        
#         return self.gaussian_matrices_cache[matrix_key]
    
#     def clear_cache(self):
#         """æ¸…ç©ºç¼“å­˜"""
#         self.dct_indices_cache.clear()
#         self.gaussian_matrices_cache.clear()

# class LUCMT(nn.Module):
#     def __init__(self, LayerNo, nf=16):
#         super().__init__()
#         self.LayerNo = LayerNo
        
#         # æ¡ä»¶æ»¤æ³¢ç½‘ç»œ
#         self.cond_filter = CondFilterV2(nf=nf)
        
#         # é‡å»ºç½‘ç»œ
#         self.fe = nn.Conv2d(1, 31, 3, padding=1)
#         self.fe2 = nn.Conv2d(1, 31, 3, padding=1)
#         self.fcs = nn.ModuleList([BasicBlock() for _ in range(LayerNo)])
        
#         # åˆ†è¾¨ç‡æ„ŸçŸ¥çš„é‡‡æ ·ç³»ç»Ÿ
#         self.sampling_system = None
    
#     def setup_sampling_system(self, device):
#         """è®¾ç½®é‡‡æ ·ç³»ç»Ÿï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
#         if self.sampling_system is None:
#             self.sampling_system = ResolutionAwareSamplingSystem(device)
    
#     def define_sampling_operators(self, x, q_G, q_DCT):
#         """ä½¿ç”¨æ–¹æ³•ä¸€çš„é‡‡æ ·ç­–ç•¥ - ä½†ä¿æŒåˆ†è¾¨ç‡æ— å…³"""
#         b, c, h, w = x.shape
#         n = h * w
        
#         # ç¡®ä¿é‡‡æ ·ç³»ç»Ÿå·²åˆå§‹åŒ–
#         self.setup_sampling_system(x.device)
        
#         # è·å–æµ‹é‡æ•°
#         q_G_val = q_G[0].item()
#         q_DCT_val = q_DCT[0].item()
        
#         # è·å–é¢„è®¡ç®—çš„æ“ä½œç¬¦
#         A_matrix_G = self.sampling_system.get_gaussian_matrix(h, w, q_G_val, q_DCT_val, b)
#         DCT_x, DCT_y = self.sampling_system.get_dct_indices(h, w, q_DCT_val)
        
#         def A(z):
#             # åˆ†åˆ«å¤„ç†ä¸¤ä¸ªé€šé“
#             z_G = z[:, 0:1]  # é«˜æ–¯åˆ†æ”¯ [b, 1, h, w]
#             z_DCT = z[:, 1:2]  # DCTåˆ†æ”¯ [b, 1, h, w]
            
#             # é«˜æ–¯åˆ†æ”¯é‡‡æ · - ä½¿ç”¨æ–¹æ³•ä¸€çš„çŸ©é˜µä¹˜æ³•
#             z_G_flat = z_G.view(b, -1)  # [b, n]
#             y_G = torch.bmm(A_matrix_G, z_G_flat.unsqueeze(-1)).squeeze(-1)
#             y_G = y_G.view(b, q_G_val, 1, 1)
            
#             # DCTåˆ†æ”¯é‡‡æ · - ä½¿ç”¨æ–¹æ³•ä¸€çš„DCTé‡‡æ ·
#             dct_coeff = dct_2d(z_DCT)
#             selected = dct_coeff[:, :, DCT_x, DCT_y]
#             y_DCT = selected.view(b, q_DCT_val, 1, 1)
            
#             return [y_G, y_DCT]
        
#         def AT(y):
#             # é«˜æ–¯é‡å»º - ä½¿ç”¨æ–¹æ³•ä¸€çš„è½¬ç½®çŸ©é˜µä¹˜æ³•
#             y_G_flat = y[0].view(b, q_G_val, 1)
#             x_G_flat = torch.bmm(A_matrix_G.transpose(1, 2), y_G_flat)
#             x_G = x_G_flat.view(b, 1, h, w)
            
#             # DCTé‡å»º - ä½¿ç”¨æ–¹æ³•ä¸€çš„é‡å»ºæ–¹å¼
#             y_DCT_flat = y[1].view(b, q_DCT_val)
            
#             # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„DCTç³»æ•°çŸ©é˜µ
#             dct_full = torch.zeros(b, 1, h, w, device=x.device)
            
#             # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•æ–¹å¼
#             for i in range(b):
#                 dct_full[i, 0, DCT_x, DCT_y] = y_DCT_flat[i]
            
#             x_DCT = idct_2d(dct_full)
            
#             return torch.cat([x_G, x_DCT], dim=1)
        
#         return A, AT

#     def forward(self, x, cs_ratio_batch):
#         b, c, h, w = x.shape
        
#         # åŠ¨æ€è®¡ç®—æµ‹é‡æ•°
#         N = h * w  # æ€»åƒç´ æ•°
#         total_measurements = int(cs_ratio_batch[0].item() * N)
        
#         # ==================== PCNetéšæœºåˆ†é…ç­–ç•¥ ====================
#         if self.training:
#             # è®­ç»ƒæ—¶ï¼šéšæœºåˆ†é…å› å­ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´å†…
#             alpha = torch.rand(1, device=x.device).item()  # Î± ~ U(0,1)
#             alpha = 0.2 + 0.6 * alpha  # é™åˆ¶åœ¨[0.2, 0.8]ä¹‹é—´ï¼Œé¿å…æç«¯åˆ†é…
            
#             q_G_val = max(1, int(total_measurements * alpha))
#             q_DCT_val = total_measurements - q_G_val
            
#             # ç¡®ä¿ä¸¤ä¸ªåˆ†æ”¯éƒ½æœ‰è‡³å°‘1ä¸ªæµ‹é‡
#             q_G_val = max(1, min(q_G_val, total_measurements - 1))
#             q_DCT_val = max(1, total_measurements - q_G_val)
            
#         else:
#             # æµ‹è¯•æ—¶ï¼šå›ºå®šæ¯”ä¾‹
#             q_G_val = max(1, int(total_measurements * 0.6))
#             q_DCT_val = total_measurements - q_G_val
            
#             # ç¡®ä¿ä¸¤ä¸ªåˆ†æ”¯éƒ½æœ‰è‡³å°‘1ä¸ªæµ‹é‡
#             q_G_val = max(1, min(q_G_val, total_measurements - 1))
#             q_DCT_val = max(1, total_measurements - q_G_val)
        
#         q_G = torch.tensor([q_G_val] * b, device=x.device)
#         q_DCT = torch.tensor([q_DCT_val] * b, device=x.device)
        
#         # è®¾ç½®CSæ¯”ç‡æ¡ä»¶
#         cs_ratio_G = (q_G / N).view(b, 1, 1, 1)
#         cs_ratio_DCT = (q_DCT / N).view(b, 1, 1, 1)
#         cs_ratio = torch.cat([cs_ratio_G, cs_ratio_DCT], dim=1)
        
#         # æ·±åº¦æ¡ä»¶æ»¤æ³¢
#         w_D, w_G = self.cond_filter(x, cs_ratio)
#         x_D = x * w_D  # DCTåˆ†æ”¯è¾“å…¥
#         x_G = x * w_G  # é«˜æ–¯åˆ†æ”¯è¾“å…¥
        
#         # å®šä¹‰é‡‡æ ·æ“ä½œç¬¦ - ç°åœ¨ä½¿ç”¨æ–¹æ³•ä¸€çš„ç­–ç•¥
#         A, AT = self.define_sampling_operators(x, q_G, q_DCT)
        
#         # åŒåˆ†æ”¯é‡‡æ ·
#         x_filtered = torch.cat([x_D, x_G], dim=1)  # [B, 2, H, W]
#         y = A(x_filtered)
        
#         # åˆå§‹åŒ–é‡å»º (ä½¿ç”¨ATæ“ä½œ)
#         x_init_dual = AT(y)  # [B, 2, H, W]
        
#         # å°†åŒé€šé“åˆå¹¶ä¸ºå•é€šé“
#         x_init = torch.mean(x_init_dual, dim=1, keepdim=True)  # [B, 1, H, W]
        
#         # é‡å»ºç½‘ç»œ
#         z_pre = self.fe(x_init)  # [B, 31, H, W]
#         z_cur = self.fe2(x_init) # [B, 31, H, W]
#         x_recon = x_init         # [B, 1, H, W]
#         for i in range(self.LayerNo):
#             x_dual = self.fcs[i](x_recon, z_pre, z_cur, mask=None, PhiTb=x_init)
#             x_recon = x_dual  # BasicBlockç°åœ¨è¾“å‡º[B, 1, H, W]
#             z_pre = z_cur
#             # ç¡®ä¿z_curæ­£ç¡®æ›´æ–°
#             if x_dual.shape[1] > 1:
#                 z_cur = x_dual[:, 1:, :, :]
#             else:
#                 # å¦‚æœx_dualåªæœ‰1é€šé“ï¼Œä½¿ç”¨fe2é‡æ–°æå–ç‰¹å¾
#                 z_cur = self.fe2(x_dual)
            
#         return x_recon

#     def clear_sampling_cache(self):
#         """æ¸…ç©ºé‡‡æ ·ç¼“å­˜ï¼ˆç”¨äºå†…å­˜ç®¡ç†ï¼‰"""
#         if self.sampling_system:
#             self.sampling_system.clear_cache()